{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d777133a",
   "metadata": {
    "cellId": "m78tog15lbpxiveuiimf5a",
    "execution_id": "1d85d7e2-c710-4d94-81bf-be369d999524"
   },
   "source": [
    "# Variable misuse detection\n",
    "\n",
    "VarMisuse at node level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177fbaa",
   "metadata": {
    "cellId": "7chovcd1n2pbd24n3juakw",
    "execution_id": "c32e0a38-e69b-45e2-b1ff-dbf0901af534"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e649f",
   "metadata": {
    "cellId": "9pcohjvr41m1gwdbllxu5p",
    "execution_id": "6abd40bc-1ef8-4c9c-8e27-7a7d25dcdaa1"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# %pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab53b30",
   "metadata": {
    "cellId": "9a2l5g300ri7o2irqhxr1",
    "execution_id": "7f208c6e-ba1f-426a-8cc6-7b4496ec15ac"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from random import random\n",
    "\n",
    "from SourceCodeTools.models.training_config import get_config, save_config, load_config\n",
    "from SourceCodeTools.code.data.dataset.Dataset import SourceGraphDataset, filter_dst_by_freq\n",
    "from SourceCodeTools.models.graph.train.sampling_multitask2 import training_procedure, SamplingMultitaskTrainer\n",
    "from SourceCodeTools.models.graph.train.objectives.NodeClassificationObjective import NodeClassifierObjective\n",
    "from SourceCodeTools.models.graph.train.utils import get_name, get_model_base\n",
    "from SourceCodeTools.models.graph import RGGAN\n",
    "from SourceCodeTools.tabular.common import compact_property\n",
    "from SourceCodeTools.code.data.file_utils import unpersist\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77fbda",
   "metadata": {
    "cellId": "g9tiww3zd86nnak57q3n4j",
    "execution_id": "953ce16f-c570-4981-b93b-afc1ec495667"
   },
   "source": [
    "# Prepare parameters and options\n",
    "\n",
    "Full list of options that can be added can be found in `SourceCodeTools/models/training_options.py`. They are ment to be used as arguments for cli trainer. Trainer script can be found in `SourceCodeTools/scripts/train.py`.\n",
    "\n",
    "There are a lot of parameters. Ones that might be of interest are marked with `***`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f990ba2",
   "metadata": {
    "cellId": "j0alxbc2u5cevzlq4hcne",
    "execution_id": "e8a42e85-4965-4057-86cd-67d8db7d3a95"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "config = get_config(\n",
    "    # tokenizer\n",
    "    tokenizer_path=\"sentencepiece_bpe.model\", # *** path to sentencepiece model\n",
    "    \n",
    "    # dataset parameters\n",
    "    data_path=\"10_percent_v1\",             # *** path to node type\n",
    "    use_node_types=False,                # node types currently not supported\n",
    "    use_edge_types=False,                 # whether to use edge types\n",
    "    filter_edges=None,                   # None or list of edge type names\n",
    "    self_loops=False,                    # whether to use self loops\n",
    "    train_frac=0.8,                      # *** fraction of nodes to use for training\n",
    "    random_seed=42,                      # random seed for splitting dataset int o train test validation\n",
    "    min_count_for_objectives=5,          # *** minimum frequency of targets\n",
    "    no_global_edges=False,               # remove global edges\n",
    "    remove_reverse=False,                # remove reverse edges\n",
    "    custom_reverse=None,                 # None or list of edges, for which reverse edges should be created (use together with `remove_reverse`)\n",
    "    \n",
    "    # training parameters\n",
    "    model_output_dir=\"10_percent_v1\",      # *** directory to save checkpoints and training data\n",
    "    batch_size=256,                     # *** \n",
    "    sampling_neighbourhood_size=10,      # number of dependencies to sample for each node\n",
    "    neg_sampling_factor=1,               # *** number of negative samples for each positive sample\n",
    "    epochs=2,                           # *** number of epochs\n",
    "    elem_emb_size=30,                   # *** dimensionality of target embeddings (for node name prediction)\n",
    "    pretraining_phase=0,                 # number of epochs for pretraining\n",
    "    embedding_table_size=200000,         # *** embedding table size for subwords\n",
    "    save_checkpoints=False,              # set to False if checkpoints are not needed\n",
    "    save_each_epoch=False,               # save each epoch, useful in case of studying model behavior\n",
    "    measure_scores=True,                 # *** measure ranking scores during evaluation\n",
    "    dilate_scores=1,                   # downsampling factor for measuring scores to make evaluation faster\n",
    "    objectives=\"node_clf\",               # type of objective\n",
    "    force_w2v_ns=True,                   # negative sampling strategy\n",
    "    gpu=-1,                              # gpuid\n",
    "    restore_state=False,\n",
    "    pretrained=None,\n",
    "    \n",
    "    # model parameters\n",
    "    node_emb_size=30,                   # *** dimensionality of node embeddings\n",
    "    h_dim=30,                           # *** should match to node dimensionality\n",
    "    n_layers=3,\n",
    "    num_bases=10,                        # number of bases for computing parmetwer weights for different edge types\n",
    "    dropout=0.2,                         # *** \n",
    "    use_self_loop=True,                  #\n",
    "    activation=\"tanh\",                   # *** \n",
    "    learning_rate=3e-3,                  # *** \n",
    "    use_gcn_checkpoint=True,\n",
    "    use_att_checkpoint=True,\n",
    "    use_gru_checkpoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a71f12",
   "metadata": {
    "cellId": "lzw7yyqpk50z9u7wcbaqc",
    "execution_id": "1309304e-abda-4115-846f-2eee4c19fb17"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "save_config(config, \"config.yaml\")\n",
    "config = load_config(\"config.yaml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817556e6",
   "metadata": {
    "cellId": "v34hwuqv65s18nug683fg7",
    "execution_id": "53a30c51-5597-424c-b2ef-071505915ad6"
   },
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506ddbb",
   "metadata": {
    "cellId": "iof1csl4j882hpr3oumv",
    "execution_id": "80b7d312-5e30-4916-a1f9-5ff6398849d8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "dataset = SourceGraphDataset(\n",
    "    **{**config[\"DATASET\"], **config[\"TOKENIZER\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1c9b1",
   "metadata": {
    "cellId": "f4jdqezhsqibhlxs979vap",
    "execution_id": "ab854bb5-2364-4b34-9258-e14152dbd554"
   },
   "source": [
    "# Declare target loading function (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440149af",
   "metadata": {
    "cellId": "rpcono88tre8eo3yhkyg34",
    "execution_id": "8c66ae29-f4f2-4eca-8611-ff95070cb5f2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "l = unpersist(\"10_percent_v1/misuse_labels.json.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e680bd",
   "metadata": {
    "cellId": "njgb5upl5dmam9ew05gs2n",
    "execution_id": "ff2b8337-afbc-45e6-a6fc-c44a760898ae"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a57b1",
   "metadata": {
    "cellId": "2xwtlmjwg4n94vvra0xqum",
    "execution_id": "1b70a3f3-0cbf-4548-b39e-6cfaba488174"
   },
   "outputs": [],
   "source": [
    "# #!g1.1\n",
    "val, counts = np.unique(l.dst, return_counts=True)\n",
    "counts[1]/counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e896906",
   "metadata": {
    "cellId": "ch70xj1jcvcknabo74wdx",
    "execution_id": "24af92a7-8ff7-4938-97d0-0392dacb1185",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def load_var_misuse():\n",
    "    from SourceCodeTools.code.data.dataset.reader import load_data\n",
    "    \n",
    "    nodes = dataset.nodes\n",
    "    \n",
    "    var_misuse_labels = unpersist(\"10_percent_v1/misuse_labels.json.bz2\").query(\"src in @node_ids\", local_dict={\"node_ids\": nodes[\"id\"]})\n",
    "    \n",
    "    return var_misuse_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66743621",
   "metadata": {
    "cellId": "ntn8bl4amtnqpahg9cygxk",
    "execution_id": "24e7d926-958b-4fc0-a478-320b5f25f9d0"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "load_var_misuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e75da",
   "metadata": {
    "cellId": "jbdmerb9bk54badmlcnrn",
    "execution_id": "a7cc54b6-50c0-4891-ad88-e88b707ce1c9"
   },
   "source": [
    "# Define objectives\n",
    "\n",
    "Currenlty objectives for node classification (`NodeClassifierObjective`), and name-based node embedding training `SubwordEmbedderObjective`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406c197",
   "metadata": {
    "cellId": "st2tlq1uuzmb8u7vx7u6",
    "execution_id": "c96a3e9a-46ef-41e5-8d92-6b8cefb39cb4"
   },
   "source": [
    "![](./figures/img1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce1075",
   "metadata": {
    "cellId": "1h4d48f56w8mbjvs78o05p",
    "execution_id": "67d7b28a-406e-4471-9489-924359e7de33"
   },
   "source": [
    "One or several objectives could be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1450b13",
   "metadata": {
    "cellId": "dljkyysdcajf77tzwxzjb5",
    "execution_id": "04c068d7-fad2-4ce7-8bbf-96bb202d1040"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Trainer(SamplingMultitaskTrainer):\n",
    "    def create_objectives(self, dataset, tokenizer_path):\n",
    "        self.objectives = nn.ModuleList()\n",
    "        self.objectives.append(\n",
    "            NodeClassifierObjective(\n",
    "                \"VarMisuse\",\n",
    "                self.graph_model, self.node_embedder, dataset.nodes,\n",
    "                load_var_misuse,                                                   # need to define this function\n",
    "                self.device, self.sampling_neighbourhood_size, self.batch_size,\n",
    "                tokenizer_path=tokenizer_path, target_emb_size=self.elem_emb_size, \n",
    "                masker=None,                                                            # masker is not needed here\n",
    "                measure_scores=False,\n",
    "                dilate_scores=self.trainer_params[\"dilate_scores\"]\n",
    "\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7099ac",
   "metadata": {
    "cellId": "s9rnusmhp86as0lg93n44",
    "execution_id": "a863451e-d404-4ffc-9e63-9879e9f5a34e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#                 base_path=self.base_path\n",
    "%tensorboard --logdir './10_percent_v1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed88325",
   "metadata": {
    "cellId": "j8vwwanf7tdlor6p7yekh",
    "execution_id": "64b25e55-f5b6-4f85-acbc-b400fd4fcb8e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "training_procedure(\n",
    "    dataset, \n",
    "    model_name=RGGAN, \n",
    "    model_params=config[\"MODEL\"],\n",
    "    trainer_params=config[\"TRAINING\"],\n",
    "    model_base_path=get_model_base(config[\"TRAINING\"], get_name(RGGAN, str(datetime.now()))),\n",
    "    tokenizer_path=config[\"TOKENIZER\"][\"tokenizer_path\"],\n",
    "    trainer=Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bcb51",
   "metadata": {
    "cellId": "6qitb7z43ngk1zy3sff69k",
    "execution_id": "126ee6c9-d3c8-41fe-9560-42eb21d95a4e"
   },
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8f59f",
   "metadata": {
    "cellId": "v8uofnppnu8lr60romrgxk",
    "execution_id": "479372a7-6ce4-4b00-8707-354ec52fd0a7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from SourceCodeTools.code.common import read_edges\n",
    "common_edges = read_edges('10_percent_v1/common_edges.json.bz2', True)\n",
    "file_id2label = dict(unpersist(\"10_percent_v1/common_filecontent.json.bz2\")[[\"id\", \"label\"]].values)\n",
    "node_id2_fileId = {}\n",
    "for edges in common_edges:\n",
    "    node_id2_fileId.update(dict(zip(edges[\"source_node_id\"], edges[\"file_id\"])))\n",
    "    node_id2_fileId.update(dict(zip(edges[\"target_node_id\"], edges[\"file_id\"])))\n",
    "    \n",
    "# sink = open(\"node_fieid_label_map_new.csv\", \"w\")\n",
    "# sink.write(\"node_id,file_id,true_label\\n\")\n",
    "# for node_id, file_id in node_id2_fileId.items():\n",
    "#     sink.write(f\"{node_id},{file_id},{file_id2label[file_id]}\\n\")\n",
    "# sink.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9287d",
   "metadata": {
    "cellId": "4b5uqhftxsshq7nnnltkic",
    "execution_id": "f035cd50-8ec7-4d8c-a0b1-d9c431f3e686"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def calc_acc():\n",
    "    \n",
    "    label_map = {'Variable misuse':'misused', 'Correct':'correct'}\n",
    "    mapping_df = pd.DataFrame([[node_id,file_id, label_map.get(file_id2label[file_id])] for node_id, file_id in node_id2_fileId.items()], columns=['node_id','file_id','true_label'])\n",
    "    \n",
    "    node_id2file_id = dict(zip(mapping_df[\"node_id\"], mapping_df[\"file_id\"]))\n",
    "    node_id2true_label = dict(zip(mapping_df[\"node_id\"], mapping_df[\"true_label\"]))\n",
    "    \n",
    "    def get_info(node_id, mapping):\n",
    "        if node_id in mapping:\n",
    "            return mapping[node_id]\n",
    "        else:\n",
    "            print(node_id)\n",
    "            raise ValueError()\n",
    "            \n",
    "    for target in ['test','val']:\n",
    "        pred_df = pd.read_csv(f'{target}_temp.txt', delimiter = \"\\t\", header=None) #(9414, 2)\n",
    "        pred_df.columns = ['id', 'pred_label']\n",
    "        pred_df[\"file_id\"] = pred_df[\"id\"].apply(lambda x: get_info(x, node_id2file_id))\n",
    "        pred_df[\"true_label\"] = pred_df[\"id\"].apply(lambda x: get_info(x, node_id2true_label))\n",
    "        print(pred_df['true_label'].value_counts())\n",
    "        print(pred_df['pred_label'].value_counts())\n",
    "        #pred_df['true_label'] = pred_df.true_label.apply(lambda x : label_map.get(x))\n",
    "        \n",
    "        correct_l = 0 \n",
    "        incorrect_l = 0\n",
    "        for name, group in pred_df.groupby('file_id'):\n",
    "            pred = 'correct'\n",
    "            if 'misused' in group.pred_label.tolist():\n",
    "                #print(group.pred_label.tolist())\n",
    "                pred = 'misused'\n",
    "            true_l = group.true_label.unique()[0]\n",
    "            #print(true_l, pred)\n",
    "            correct_l += int(true_l == pred)\n",
    "            incorrect_l += int(true_l != pred) \n",
    "            \n",
    "        print(f'{target} accuracy {correct_l/(correct_l+incorrect_l)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb12e2e",
   "metadata": {
    "cellId": "pv18l1wpomtlkipm9st1",
    "execution_id": "4fcdb05d-c1dd-4fea-9e5a-0d7ddfccb934"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%%time\n",
    "calc_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f23bf7",
   "metadata": {
    "cellId": "jcj0wki24t8zgxidvot77a",
    "execution_id": "91df278c-b6c5-47d6-ae6a-3602145444cc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SourceCodeTools",
   "language": "python",
   "name": "sourcecodetools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notebookId": "2097dc27-1489-4e1c-b5e2-a57bf5d7b94c",
  "notebookPath": "method-embedding/examples/VarMisuse.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

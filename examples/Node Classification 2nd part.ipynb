{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6f7651-7021-4815-8b4b-fdecf036168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40139714-ddb0-49a7-89df-6e2db068185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "from SourceCodeTools.code.data.dataset.Dataset import SourceGraphDataset, filter_dst_by_freq\n",
    "from SourceCodeTools.models.graph.train.sampling_multitask2 import training_procedure, SamplingMultitaskTrainer\n",
    "from SourceCodeTools.models.graph.train.objectives.NodeClassificationObjective import NodeClassifierObjective\n",
    "from SourceCodeTools.models.graph.train.utils import get_name, get_model_base\n",
    "from SourceCodeTools.models.graph import RGGAN\n",
    "from SourceCodeTools.tabular.common import compact_property\n",
    "from SourceCodeTools.code.data.file_utils import unpersist\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from torch import nn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53034bd-85f1-49a1-af78-4158b3f846bf",
   "metadata": {},
   "source": [
    "# Prepare parameters and options\n",
    "\n",
    "Full list of options that can be added can be found in `SourceCodeTools/models/training_options.py`. They are ment to be used as arguments for cli trainer. Trainer script can be found in `SourceCodeTools/scripts/train.py`.\n",
    "\n",
    "There are a lot of parameters. Ones that might be of interest are marked with `***`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05dd3bb-e223-423a-9e24-b9aa281e08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    \n",
    "    # dataset parameters\n",
    "    data_path=\"large_graph\",             # *** path to node type\n",
    "    model_output_dir=\"large_graph\",      # *** directory to save checkpoints and training data\n",
    "    use_node_types=False,                # node types currently not supported\n",
    "    use_edge_types=True,                 # whether to use edge types\n",
    "    filter_edges=None,                   # None or list of edge type names\n",
    "    self_loops=False,                    # whether to use self loops\n",
    "    train_frac=0.8,                      # *** fraction of nodes to use for training\n",
    "    tokenizer=\"sentencepiece_bpe.model\", # *** path to sentencepiece model\n",
    "    random_seed=42,                      # random seed for splitting dataset int o train test validation\n",
    "    min_count_for_objectives=5,          # *** minimum frequency of targets\n",
    "    no_global_edges=False,               # remove global edges\n",
    "    remove_reverse=False,                # remove reverse edges\n",
    "    custom_reverse=None,                 # None or list of edges, for which reverse edges should be created (use together with `remove_reverse`)\n",
    "    \n",
    "    # training parameters\n",
    "    batch_size=1024,                     # *** \n",
    "    num_per_neigh=10,                    # number of dependencies to sample for each node\n",
    "    neg_sampling_factor=1,               # *** number of negative samples for each positive sample\n",
    "    epochs=10,                           # *** number of epochs\n",
    "    node_emb_size=100,                   # *** dimensionality of node embeddings\n",
    "    elem_emb_size=100,                   # *** dimensionality of target embeddings (for node name prediction)\n",
    "    pretraining_phase=0,                 # number of epochs for pretraining\n",
    "    use_layer_scheduling=False,          # currently not supported\n",
    "    schedule_layers_every=1,             # currently not supported\n",
    "    embedding_table_size=200000,         # *** embedding table size for subwords\n",
    "    save_checkpoints=False,              # set to False if checkpoints are not needed\n",
    "    save_each_epoch=False,               # save each epoch, useful in case of studying model behavior\n",
    "    measure_scores=True,                 # *** measure ranking scores during evaluation\n",
    "    dilate_scores=200,                   # downsampling factor for measuring scores to make evaluation faster\n",
    "    objectives=\"node_clf\",               # type of objective\n",
    "    early_stopping=False,                # whether to do early stopping\n",
    "    early_stopping_tolerance=1e-5,\n",
    "    force_w2v_ns=True,                   # negative sampling strategy\n",
    "    metric=\"inner_prod\",                 # do not change for now\n",
    "    nn_index=\"brute\",                    # do not change for now\n",
    "    gpu=-1,                              # gpuid\n",
    "    use_gcn_checkpoint=True,\n",
    "    use_att_checkpoint=True,\n",
    "    use_gru_checkpoint=True,\n",
    "    restore_state=False,\n",
    "    pretrained=None\n",
    ")\n",
    "\n",
    "model_parameters = {\n",
    "    'h_dim': 100,                        # *** should match to node dimensionality\n",
    "    'num_bases': 10,                     # number of bases for computing parmetwer weights for different edge types\n",
    "    'dropout': 0.2,                      # *** \n",
    "    'use_self_loop': True,               #\n",
    "    'activation': torch.tanh,            # *** \n",
    "    'lr': 1e-3                           # *** \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902b596-ddb3-4df7-bbcd-f18a370b7e5f",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dab34f0-106e-4649-a5fa-6251bbc338ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Random state for splitting dataset is fixed\n"
     ]
    }
   ],
   "source": [
    "dataset = SourceGraphDataset(\n",
    "    args.data_path,\n",
    "    use_node_types=args.use_node_types,\n",
    "    use_edge_types=args.use_edge_types,\n",
    "    filter=args.filter_edges,\n",
    "    self_loops=args.self_loops,\n",
    "    train_frac=args.train_frac,\n",
    "    tokenizer_path=args.tokenizer,         # path to sentencepiece tokenizer\n",
    "    random_seed=args.random_seed,\n",
    "    min_count_for_objectives=args.min_count_for_objectives,\n",
    "    no_global_edges=args.no_global_edges,\n",
    "    remove_reverse=args.remove_reverse,\n",
    "    custom_reverse=args.custom_reverse,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bb9ea-d2d3-471e-b77f-3257bfce4796",
   "metadata": {},
   "source": [
    "# Declare target loading function (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d355768e-8097-4f94-9ff8-b330888d57bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_type_prediction():\n",
    "    from SourceCodeTools.code.data.dataset.reader import load_data\n",
    "    \n",
    "    nodes, edges = dataset.nodes, dataset.edges\n",
    "    \n",
    "    type_ann = unpersist(\"large_graph/type_annotations.json.bz2\").query(\"src in @node_ids\", local_dict={\"node_ids\": nodes[\"id\"]})\n",
    "    \n",
    "    norm = lambda x: x.strip(\"\\\"\").strip(\"'\").split(\"[\")[0].split(\".\")[-1]\n",
    "\n",
    "    type_ann[\"dst\"] = type_ann[\"dst\"].apply(norm)\n",
    "    type_ann = filter_dst_by_freq(type_ann, args.min_count_for_objectives)\n",
    "    type_ann = type_ann[[\"src\", \"dst\"]]\n",
    "\n",
    "    return type_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca39c6-66ad-4f7f-9e6e-dfe8a30bbe66",
   "metadata": {},
   "source": [
    "# Define objectives\n",
    "\n",
    "Currenlty objectives for node classification (`NodeClassifierObjective`), and name-based node embedding training `SubwordEmbedderObjective`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e327eb-cb53-41de-a420-9417db847336",
   "metadata": {},
   "source": [
    "![](\"examples/figures/img1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74c8af-62ac-4347-9c15-cb67a514e615",
   "metadata": {},
   "source": [
    "One or several objectives could be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a01096-6c79-4eae-bd92-d90e93f8df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(SamplingMultitaskTrainer):\n",
    "    def create_objectives(self, dataset, tokenizer_path):\n",
    "        self.objectives = nn.ModuleList()\n",
    "        \n",
    "        self.objectives.append(\n",
    "            NodeClassifierObjective(\n",
    "                \"NodeTypeClassifier\",\n",
    "                self.graph_model, self.node_embedder, dataset.nodes,\n",
    "                dataset.load_node_classes,                                              # need to define this function\n",
    "                self.device, self.sampling_neighbourhood_size, self.batch_size,\n",
    "                tokenizer_path=tokenizer_path, target_emb_size=self.elem_emb_size,\n",
    "                masker=dataset.create_node_clf_masker(),                                # this is needed only for node type classification\n",
    "                measure_scores=self.trainer_params[\"measure_scores\"],\n",
    "                dilate_scores=self.trainer_params[\"dilate_scores\"]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # self.objectives.append(\n",
    "        #     NodeClassifierObjective(\n",
    "        #         \"TypeAnnPrediction\",\n",
    "        #         self.graph_model, self.node_embedder, dataset.nodes,\n",
    "        #         load_type_prediction,                                                   # need to define this function\n",
    "        #         self.device, self.sampling_neighbourhood_size, self.batch_size,\n",
    "        #         tokenizer_path=tokenizer_path, target_emb_size=self.elem_emb_size, \n",
    "        #         masker=None,                                                            # masker is not needed here\n",
    "        #         measure_scores=self.trainer_params[\"measure_scores\"],\n",
    "        #         dilate_scores=self.trainer_params[\"dilate_scores\"]\n",
    "        #     )\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3bf67e8-162f-467b-9a6b-72c55f69e845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-52fe3ab54eacdd06\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-52fe3ab54eacdd06\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"large_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f133d1b-4e5a-4642-a4ce-92965d12b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes 324218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████| 243/243 [01:56<00:00,  2.09it/s]\n",
      "Epoch 1:   0%|▏                                 | 1/243 [00:00<00:43,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 130 s\n",
      "{'Accuracy/test/NodeTypeClassifier_': 0.3872578845046083,\n",
      " 'Accuracy/train/NodeTypeClassifier_': 0.4505928853754941,\n",
      " 'Accuracy/train_avg/NodeTypeClassifier': 0.24216100467232712,\n",
      " 'Accuracy/val/NodeTypeClassifier_': 0.3865718884881872,\n",
      " 'Loss/test/NodeTypeClassifier_': 2.430450024143342,\n",
      " 'Loss/train/NodeTypeClassifier_': 1.7834914922714233,\n",
      " 'Loss/train_avg/NodeTypeClassifier': 2.880048046877355,\n",
      " 'Loss/val/NodeTypeClassifier_': 2.4204529446940266,\n",
      " 'acc@1/test/NodeTypeClassifier_': 0.3330078125,\n",
      " 'acc@1/val/NodeTypeClassifier_': 0.3564453125,\n",
      " 'acc@10/test/NodeTypeClassifier_': 0.685546875,\n",
      " 'acc@10/val/NodeTypeClassifier_': 0.7041015625,\n",
      " 'acc@3/test/NodeTypeClassifier_': 0.52734375,\n",
      " 'acc@3/val/NodeTypeClassifier_': 0.5419921875,\n",
      " 'acc@5/test/NodeTypeClassifier_': 0.5849609375,\n",
      " 'acc@5/val/NodeTypeClassifier_': 0.60546875,\n",
      " 'ndcg@1/test/NodeTypeClassifier_': 0.3330078125,\n",
      " 'ndcg@1/val/NodeTypeClassifier_': 0.3564453125,\n",
      " 'ndcg@10/test/NodeTypeClassifier_': 0.509616803760443,\n",
      " 'ndcg@10/val/NodeTypeClassifier_': 0.5287765886580593,\n",
      " 'ndcg@3/test/NodeTypeClassifier_': 0.4522957493826779,\n",
      " 'ndcg@3/val/NodeTypeClassifier_': 0.46980438508301237,\n",
      " 'ndcg@5/test/NodeTypeClassifier_': 0.4768105449592973,\n",
      " 'ndcg@5/val/NodeTypeClassifier_': 0.4966714895903787}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 243/243 [02:22<00:00,  1.71it/s]\n",
      "Epoch 2:   0%|▏                                 | 1/243 [00:00<00:42,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time: 155 s\n",
      "{'Accuracy/test/NodeTypeClassifier_': 0.6683143721198157,\n",
      " 'Accuracy/train/NodeTypeClassifier_': 0.6719367588932806,\n",
      " 'Accuracy/train_avg/NodeTypeClassifier': 0.4488894645427707,\n",
      " 'Accuracy/val/NodeTypeClassifier_': 0.6736592268665758,\n",
      " 'Loss/test/NodeTypeClassifier_': 1.2975636951385006,\n",
      " 'Loss/train/NodeTypeClassifier_': 1.2128385305404663,\n",
      " 'Loss/train_avg/NodeTypeClassifier': 2.06057782992414,\n",
      " 'Loss/val/NodeTypeClassifier_': 1.2764978331904258,\n",
      " 'acc@1/test/NodeTypeClassifier_': 0.6044921875,\n",
      " 'acc@1/val/NodeTypeClassifier_': 0.6171875,\n",
      " 'acc@10/test/NodeTypeClassifier_': 0.7880859375,\n",
      " 'acc@10/val/NodeTypeClassifier_': 0.7939453125,\n",
      " 'acc@3/test/NodeTypeClassifier_': 0.689453125,\n",
      " 'acc@3/val/NodeTypeClassifier_': 0.7099609375,\n",
      " 'acc@5/test/NodeTypeClassifier_': 0.712890625,\n",
      " 'acc@5/val/NodeTypeClassifier_': 0.7294921875,\n",
      " 'ndcg@1/test/NodeTypeClassifier_': 0.6044921875,\n",
      " 'ndcg@1/val/NodeTypeClassifier_': 0.6171875,\n",
      " 'ndcg@10/test/NodeTypeClassifier_': 0.6880186110817406,\n",
      " 'ndcg@10/val/NodeTypeClassifier_': 0.7005450259941073,\n",
      " 'ndcg@3/test/NodeTypeClassifier_': 0.6534935717110785,\n",
      " 'ndcg@3/val/NodeTypeClassifier_': 0.6718851894356883,\n",
      " 'ndcg@5/test/NodeTypeClassifier_': 0.6631595872241379,\n",
      " 'ndcg@5/val/NodeTypeClassifier_': 0.6799544679071307}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  16%|█████▎                           | 39/243 [00:15<01:29,  2.28it/s]"
     ]
    }
   ],
   "source": [
    "training_procedure(\n",
    "    dataset, \n",
    "    model_name=RGGAN, \n",
    "    model_params=model_parameters,\n",
    "    args=args,\n",
    "    model_base_path=get_model_base(args, get_name(RGGAN, str(datetime.now()))),\n",
    "    trainer=Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10d961-6706-4469-a5e9-b376e976e1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536c8db-a255-4bb4-a41f-a002c21d344e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SourceCodeTools]",
   "language": "python",
   "name": "conda-env-SourceCodeTools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "from SourceCodeTools.models.training_config import get_config, save_config, load_config\n",
    "from SourceCodeTools.code.data.dataset.Dataset import SourceGraphDataset, filter_dst_by_freq\n",
    "from SourceCodeTools.models.graph.train.sampling_multitask2 import training_procedure, SamplingMultitaskTrainer\n",
    "from SourceCodeTools.models.graph.train.objectives.NodeClassificationObjective import NodeClassifierObjective\n",
    "from SourceCodeTools.models.graph.train.utils import get_name, get_model_base\n",
    "from SourceCodeTools.models.graph import RGGAN\n",
    "from SourceCodeTools.tabular.common import compact_property\n",
    "from SourceCodeTools.code.data.file_utils import unpersist\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare parameters and options\n",
    "\n",
    "Full list of options that can be added can be found in `SourceCodeTools/models/training_options.py`. They are ment to be used as arguments for cli trainer. Trainer script can be found in `SourceCodeTools/scripts/train.py`.\n",
    "\n",
    "There are a lot of parameters. Ones that might be of interest are marked with `***`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\n",
    "    # tokenizer\n",
    "    tokenizer_path=\"sentencepiece_bpe.model\", # *** path to sentencepiece model\n",
    "    \n",
    "    # dataset parameters\n",
    "    data_path=\"large_graph\",             # *** path to node type\n",
    "    use_node_types=False,                # node types currently not supported\n",
    "    use_edge_types=True,                 # whether to use edge types\n",
    "    filter_edges=None,                   # None or list of edge type names\n",
    "    self_loops=False,                    # whether to use self loops\n",
    "    train_frac=0.8,                      # *** fraction of nodes to use for training\n",
    "    random_seed=42,                      # random seed for splitting dataset int o train test validation\n",
    "    min_count_for_objectives=5,          # *** minimum frequency of targets\n",
    "    no_global_edges=False,               # remove global edges\n",
    "    remove_reverse=False,                # remove reverse edges\n",
    "    custom_reverse=None,                 # None or list of edges, for which reverse edges should be created (use together with `remove_reverse`)\n",
    "    partition=\"large_graph/partition.json.bz2\",  # partition into train/test/val\n",
    "    \n",
    "    # training parameters\n",
    "    model_output_dir=\"large_graph\",      # *** directory to save checkpoints and training data\n",
    "    batch_size=128,                     # *** \n",
    "    sampling_neighbourhood_size=10,      # number of dependencies to sample for each node\n",
    "    neg_sampling_factor=1,               # *** number of negative samples for each positive sample\n",
    "    epochs=10,                           # *** number of epochs\n",
    "    elem_emb_size=100,                   # *** dimensionality of target embeddings (for node name prediction)\n",
    "    pretraining_phase=0,                 # number of epochs for pretraining\n",
    "    embedding_table_size=200000,         # *** embedding table size for subwords\n",
    "    save_checkpoints=False,              # set to False if checkpoints are not needed\n",
    "    save_each_epoch=False,               # save each epoch, useful in case of studying model behavior\n",
    "    measure_scores=True,                 # *** measure ranking scores during evaluation\n",
    "    dilate_scores=200,                   # downsampling factor for measuring scores to make evaluation faster\n",
    "    objectives=\"node_clf\",               # type of objective\n",
    "    force_w2v_ns=True,                   # negative sampling strategy\n",
    "    gpu=-1,                              # gpuid\n",
    "    restore_state=False,\n",
    "    pretrained=None,\n",
    "\n",
    "    # model parameters\n",
    "    node_emb_size=100,                   # *** dimensionality of node embeddings\n",
    "    h_dim=100,                           # *** should match to node dimensionality\n",
    "    num_bases=10,                        # number of bases for computing parmetwer weights for different edge types\n",
    "    dropout=0.2,                         # *** \n",
    "    use_self_loop=True,                  #\n",
    "    activation=\"tanh\",                   # *** \n",
    "    learning_rate=1e-3,                  # *** \n",
    "    use_gcn_checkpoint=True,\n",
    "    use_att_checkpoint=True,\n",
    "    use_gru_checkpoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_config(config, \"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = load_config(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'DATASET': {'data_path': 'large_graph',\n  'train_frac': 0.8,\n  'filter_edges': None,\n  'min_count_for_objectives': 5,\n  'self_loops': False,\n  'use_node_types': False,\n  'use_edge_types': True,\n  'no_global_edges': False,\n  'remove_reverse': False,\n  'custom_reverse': None,\n  'restricted_id_pool': None,\n  'random_seed': 42,\n  'subgraph_id_column': 'mentioned_in',\n  'subgraph_partition': None,\n  'partition': 'large_graph/partition.json.bz2'},\n 'TRAINING': {'model': 'RGCN',\n  'model_output_dir': 'large_graph',\n  'pretrained': None,\n  'pretraining_phase': 0,\n  'sampling_neighbourhood_size': 10,\n  'neg_sampling_factor': 1,\n  'use_layer_scheduling': False,\n  'schedule_layers_every': 10,\n  'elem_emb_size': 100,\n  'embedding_table_size': 200000,\n  'epochs': 10,\n  'batch_size': 128,\n  'learning_rate': 0.001,\n  'objectives': 'node_clf',\n  'save_each_epoch': False,\n  'save_checkpoints': False,\n  'early_stopping': False,\n  'early_stopping_tolerance': 20,\n  'force_w2v_ns': True,\n  'use_ns_groups': False,\n  'nn_index': 'brute',\n  'metric': 'inner_prod',\n  'measure_scores': True,\n  'dilate_scores': 200,\n  'gpu': -1,\n  'external_dataset': None,\n  'restore_state': False,\n  'skip_final_eval': False,\n  'inference_ids_path': None},\n 'MODEL': {'node_emb_size': 100,\n  'h_dim': 100,\n  'n_layers': 5,\n  'use_self_loop': True,\n  'use_gcn_checkpoint': True,\n  'use_att_checkpoint': True,\n  'use_gru_checkpoint': True,\n  'num_bases': 10,\n  'dropout': 0.2,\n  'activation': 'tanh'},\n 'TOKENIZER': {'tokenizer_path': 'sentencepiece_bpe.model'}}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SourceGraphDataset(\n",
    "    **{**config[\"DATASET\"], **config[\"TOKENIZER\"]},\n",
    ")\n",
    "ntypes, etypes = dataset.get_graph_types()\n",
    "config[\"TRAINING\"]['ntypes'] = ntypes\n",
    "config[\"TRAINING\"]['etypes'] = etypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare target loading function (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_type_prediction():\n",
    "    return unpersist(\"large_graph/type_annotations.json.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define objective\n",
    "\n",
    "Currenlty objectives for node classification (`NodeClassifierObjective`), and name-based node embedding training `SubwordEmbedderObjective`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/VitalyRomanov/method-embedding/raw/master/examples/figures/img1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SourceCodeTools.models.graph.train.objectives.NodeClassificationObjective import ClassifierTargetMapper\n",
    "\n",
    "\n",
    "class Trainer(SamplingMultitaskTrainer):\n",
    "    def create_objectives(self, dataset, tokenizer_path):\n",
    "        self.objectives = nn.ModuleList()\n",
    "        \n",
    "        self.objectives.append(\n",
    "            self._create_node_level_objective(\n",
    "                objective_name=\"TypeAnnPrediction\",\n",
    "                objective_class=NodeClassifierObjective,\n",
    "                label_loader_class=ClassifierTargetMapper,\n",
    "                dataset=dataset,\n",
    "                labels_fn=load_type_prediction,  # need to define this function\n",
    "                tokenizer_path=tokenizer_path,\n",
    "                masker_fn=dataset.create_subword_masker,\n",
    "                preload_for=\"file\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir \"large_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "training_procedure(\n",
    "    dataset, \n",
    "    model_name=RGGAN, \n",
    "    model_params=config[\"MODEL\"],\n",
    "    trainer_params=config[\"TRAINING\"],\n",
    "    model_base_path=get_model_base(config[\"TRAINING\"], get_name(RGGAN, str(datetime.now()))),\n",
    "    tokenizer_path=config[\"TOKENIZER\"][\"tokenizer_path\"],\n",
    "    trainer=Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SourceCodeTools",
   "language": "python",
   "name": "sourcecodetools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from argparse import Namespace\n",
    "from SourceCodeTools.nlp.codebert.codebert_train import CodeBertModelTrainer2, test_step, train_step_finetune, CodebertHybridModel, batch_to_torch\n",
    "from SourceCodeTools.nlp.entity.type_prediction import scorer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from time import time\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(dataset_path, partition):\n",
    "    data_path = join(dataset_path, f\"var_misuse_seq_{partition}.json\")\n",
    "    \n",
    "    data = []\n",
    "    for line in open(data_path, \"r\"):\n",
    "        entry = json.loads(line)\n",
    "        \n",
    "        text = entry.pop(\"text\")\n",
    "        data.append((text, entry))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariableMisuseDetector(CodeBertModelTrainer2):\n",
    "    def get_trial_dir(self):\n",
    "        return os.path.join(self.output_dir, \"codebert_var_mususe\" + str(datetime.now())).replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "    \n",
    "    def train(\n",
    "            self, model, train_batches, test_batches, epochs, report_every=10, scorer=None, learning_rate=0.01,\n",
    "            learning_rate_decay=1., finetune=False, summary_writer=None, save_ckpt_fn=None, no_localization=False\n",
    "    ):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learning_rate_decay)\n",
    "\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_f1s = []\n",
    "        test_f1s = []\n",
    "\n",
    "        num_train_batches = len(train_batches)\n",
    "        num_test_batches = len(test_batches)\n",
    "\n",
    "        best_f1 = 0.\n",
    "\n",
    "        for e in range(epochs):\n",
    "            losses = []\n",
    "            ps = []\n",
    "            rs = []\n",
    "            f1s = []\n",
    "\n",
    "            start = time()\n",
    "            model.train()\n",
    "\n",
    "            for ind, batch in enumerate(tqdm(train_batches)):\n",
    "                batch_to_torch(batch, self.device)\n",
    "                # token_ids, graph_ids, labels, class_weights, lengths = b\n",
    "                loss, p, r, f1 = train_step_finetune(\n",
    "                    model=model, optimizer=optimizer, token_ids=batch['tok_ids'],\n",
    "                    prefix=batch['prefix'], suffix=batch['suffix'], graph_ids=batch['graph_ids'],\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=batch['no_loc_mask'] if no_localization else batch['hide_mask'],\n",
    "                    # class_weights=batch['class_weights'],\n",
    "                    scorer=scorer, finetune=finetune and e / epochs > 0.6,\n",
    "                    vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "                losses.append(loss.cpu().item())\n",
    "                ps.append(p)\n",
    "                rs.append(r)\n",
    "                f1s.append(f1)\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Train\", loss, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Train\", p, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Train\", r, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Train\", f1, global_step=e * num_train_batches + ind)\n",
    "\n",
    "            test_alosses = []\n",
    "            test_aps = []\n",
    "            test_ars = []\n",
    "            test_af1s = []\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for ind, batch in enumerate(test_batches):\n",
    "                batch_to_torch(batch, self.device)\n",
    "                # token_ids, graph_ids, labels, class_weights, lengths = b\n",
    "                test_loss, test_p, test_r, test_f1 = test_step(\n",
    "                    model=model, token_ids=batch['tok_ids'],\n",
    "                    prefix=batch['prefix'], suffix=batch['suffix'], graph_ids=batch['graph_ids'],\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=batch['no_loc_mask'] if no_localization else batch['hide_mask'],\n",
    "                    # class_weights=batch['class_weights'],\n",
    "                    scorer=scorer, vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Test\", test_loss, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Test\", test_p, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Test\", test_r, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Test\", test_f1, global_step=e * num_test_batches + ind)\n",
    "                test_alosses.append(test_loss.cpu().item())\n",
    "                test_aps.append(test_p)\n",
    "                test_ars.append(test_r)\n",
    "                test_af1s.append(test_f1)\n",
    "\n",
    "            epoch_time = time() - start\n",
    "\n",
    "            train_losses.append(float(sum(losses) / len(losses)))\n",
    "            train_f1s.append(float(sum(f1s) / len(f1s)))\n",
    "            test_losses.append(float(sum(test_alosses) / len(test_alosses)))\n",
    "            test_f1s.append(float(sum(test_af1s) / len(test_af1s)))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {e}, {epoch_time: .2f} s, Train Loss: {train_losses[-1]: .4f}, Train P: {sum(ps) / len(ps): .4f}, Train R: {sum(rs) / len(rs): .4f}, Train F1: {sum(f1s) / len(f1s): .4f}, \"\n",
    "                f\"Test loss: {test_losses[-1]: .4f}, Test P: {sum(test_aps) / len(test_aps): .4f}, Test R: {sum(test_ars) / len(test_ars): .4f}, Test F1: {test_f1s[-1]: .4f}\")\n",
    "\n",
    "            if save_ckpt_fn is not None and float(test_f1s[-1]) > best_f1:\n",
    "                save_ckpt_fn()\n",
    "                best_f1 = float(test_f1s[-1])\n",
    "\n",
    "            scheduler.step(epoch=e)\n",
    "\n",
    "        return train_losses, train_f1s, test_losses, test_f1s\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        model_params = copy(self.model_params)\n",
    "\n",
    "        print(f\"\\n\\n{model_params}\")\n",
    "        lr = model_params.pop(\"learning_rate\")\n",
    "        lr_decay = model_params.pop(\"learning_rate_decay\")\n",
    "        suffix_prefix_buckets = model_params.pop(\"suffix_prefix_buckets\")\n",
    "\n",
    "        graph_emb = load_pkl_emb(self.graph_emb_path) if self.graph_emb_path is not None else None\n",
    "\n",
    "        train_batcher, test_batcher = self.get_dataloaders(word_emb=None, graph_emb=None, suffix_prefix_buckets=suffix_prefix_buckets)\n",
    "\n",
    "        codebert_model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        model = CodebertHybridModel(\n",
    "            codebert_model, graph_emb=None, padding_idx=0, num_classes=train_batcher.num_classes(),\n",
    "            no_graph=self.no_graph\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if self.use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        trial_dir = self.get_trial_dir()\n",
    "        os.mkdir(trial_dir)\n",
    "        self.create_summary_writer(trial_dir)\n",
    "        \n",
    "        pickle.dump(train_batcher.tagmap, open(os.path.join(trial_dir, \"tag_types.pkl\"), \"wb\"))\n",
    "\n",
    "        def save_ckpt_fn():\n",
    "            checkpoint_path = os.path.join(trial_dir, \"checkpoint\")\n",
    "            torch.save(model, open(checkpoint_path, 'wb'))\n",
    "\n",
    "        train_losses, train_f1, test_losses, test_f1 = self.train(\n",
    "            model=model, train_batches=train_batcher, test_batches=test_batcher,\n",
    "            epochs=self.epochs, learning_rate=lr,\n",
    "            scorer=lambda pred, true: scorer(pred, true, train_batcher.tagmap, no_localization=self.no_localization),\n",
    "            learning_rate_decay=lr_decay, finetune=self.finetune, save_ckpt_fn=save_ckpt_fn,\n",
    "            no_localization=self.no_localization\n",
    "        )\n",
    "\n",
    "        metadata = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"learning_rate_decay\": lr_decay,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"suffix_prefix_buckets\": suffix_prefix_buckets,\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"no_localization\": self.no_localization\n",
    "        }\n",
    "\n",
    "        print(\"Maximum f1:\", max(test_f1))\n",
    "\n",
    "        metadata.update(model_params)\n",
    "\n",
    "        with open(os.path.join(trial_dir, \"params.json\"), \"w\") as metadata_sink:\n",
    "            metadata_sink.write(json.dumps(metadata, indent=4))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/ltv/Documents/source-graph/python/variable_misuse_graph_2_percent_misuse_edges\"\n",
    "\n",
    "args = Namespace()\n",
    "args.__dict__.update({\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_seq_len\": 512,\n",
    "    \"random_seed\": 42,\n",
    "    \"epochs\": 100,\n",
    "    \"gpu\": -1,\n",
    "    # do not change items below\n",
    "    \"batch_size\": 8,\n",
    "    \"no_graph\": True,\n",
    "    \"model_output\": dataset_path,\n",
    "    \"no_localization\": False,\n",
    "    \"graph_emb_path\": None,\n",
    "    \"word_emb_path\": None,\n",
    "    \"finetune\": False,\n",
    "    \"trials\": 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(dataset_path, \"train\")\n",
    "test_data = read_data(dataset_path, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VariableMisuseDetector(\n",
    "    train_data, test_data, params={\"learning_rate\": 1e-4, \"learning_rate_decay\": 0.99, \"suffix_prefix_buckets\": 1},\n",
    "    graph_emb_path=args.graph_emb_path, word_emb_path=args.word_emb_path,\n",
    "    output_dir=args.model_output, epochs=args.epochs, batch_size=args.batch_size, gpu_id=args.gpu,\n",
    "    finetune=args.finetune, trials=args.trials, seq_len=args.max_seq_len, no_localization=args.no_localization,\n",
    "    no_graph=args.no_graph\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'learning_rate': 0.0001, 'learning_rate_decay': 0.99, 'suffix_prefix_buckets': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>def factors(self, v, w, x, z, A):ĊĠĠĠ \"\\nĠĠĠĠĠĠ...\" with entities \"[(715, 723, 'new_var'), (920, 929, 'new_var'), (10...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>def fetchmany(self, size=None):ĊĠĠĠ 'Fetches th...\" with entities \"[(555, 556, 'misuse')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>@staticmethodĊdef error(_type, msg, xml=None):Ċ...\" with entities \"[(27, 32, 'arg'), (34, 37, 'arg'), (39, 42, 'arg')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>def FetchRequestsAndResponses(self, session_id)...\" with entities \"[(531, 532, 'misuse')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>def load_complex(self, cnf, typ='', metadata_co...\" with entities \"[(222, 226, 'misuse')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>def test_async_query(cloud_config, capsys):ĊĠĠĠ...\" with entities \"[(330, 331, 'misuse')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>def match_tag_regex(_oid, data, key, term):ĊĠĠĠ...\" with entities \"[(23, 27, 'arg'), (29, 33, 'arg'), (35, 38, 'arg')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>def get_sanitized_file(url):ĊĠĠĠ (_, afile) = o...\" with entities \"[(189, 190, 'misuse')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>@larch.ValidateLarchPluginĊdef _plot_axvline(x,...\" with entities \"[(569, 575, 'misuse')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n",
      "/home/ltv/dev/method-embedding/SourceCodeTools/code/annotator_utils.py:11: UserWarning: [W030] Some entities could not be aligned in the text \"<s>@ValidateLarchPluginĊdef atomic_number(element,...\" with entities \"[(141, 147, 'misuse')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  ent_tags = spacy_biluo_tags_from_offsets(doc, ents)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_264480/3471272225.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_264480/2317739560.py\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0mgraph_emb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_pkl_emb\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgraph_emb_path\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgraph_emb_path\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 112\u001B[0;31m         \u001B[0mtrain_batcher\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_batcher\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_dataloaders\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword_emb\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgraph_emb\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuffix_prefix_buckets\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msuffix_prefix_buckets\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    113\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m         \u001B[0mcodebert_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mRobertaModel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"microsoft/codebert-base\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/nlp/codebert/codebert_train.py\u001B[0m in \u001B[0;36mget_dataloaders\u001B[0;34m(self, word_emb, graph_emb, suffix_prefix_buckets)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocab_mapping\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtok_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 159\u001B[0;31m         train_batcher = self.get_batcher(\n\u001B[0m\u001B[1;32m    160\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseq_len\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseq_len\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    161\u001B[0m             \u001B[0mgraphmap\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgraph_emb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mind\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mgraph_emb\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/nlp/codebert/codebert_extract.py\u001B[0m in \u001B[0;36mget_batcher\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_batcher\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m\"tokenizer\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"codebert\"\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatcher\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/nlp/batchers/PythonBatcher.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, batch_size, seq_len, wordmap, graphmap, tagmap, mask_unlabeled_declarations, class_weights, element_hash_size, len_sort, tokenizer, no_localization)\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_tokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtagmap\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagmap\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtag_map_from_sentences\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprepare_sent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     69\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagmap\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtagmap\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/nlp/batchers/PythonBatcher.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_tokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtagmap\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagmap\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtag_map_from_sentences\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprepare_sent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     69\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagmap\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtagmap\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/nlp/batchers/PythonBatcher.py\u001B[0m in \u001B[0;36mprepare_sent\u001B[0;34m(self, sent)\u001B[0m\n\u001B[1;32m    134\u001B[0m         \u001B[0mrepl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mannotations\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'replacements'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmask_unlabeled_declarations\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 136\u001B[0;31m             \u001B[0munlabeled_dec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfilter_unlabeled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ments\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mget_declarations\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    137\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m         \u001B[0mtokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdoc\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/code/ast/ast_tools.py\u001B[0m in \u001B[0;36mget_declarations\u001B[0;34m(function_)\u001B[0m\n\u001B[1;32m     94\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdesc\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0madded\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 96\u001B[0;31m                     \u001B[0mmentions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_mentions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     97\u001B[0m                     \u001B[0mvalid_mentions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mmention\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mmention\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmentions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m                     \u001B[0mdeclarations\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalid_mentions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/code/ast/ast_tools.py\u001B[0m in \u001B[0;36mget_mentions\u001B[0;34m(function, root, mention)\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mast\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;31m# a variable or a ...\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mid\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mmention\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m                 offset = to_offsets(function,\n\u001B[0m\u001B[1;32m     20\u001B[0m                                     [(node.lineno-1, node.end_lineno-1, node.col_offset, node.end_col_offset, \"mention\")], as_bytes=True)\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/code/annotator_utils.py\u001B[0m in \u001B[0;36mto_offsets\u001B[0;34m(body, entities, as_bytes, cum_lens, b2c)\u001B[0m\n\u001B[1;32m     79\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mas_bytes\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mb2c\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 81\u001B[0;31m             \u001B[0mb2c\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_byte_to_char_map\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbody\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     82\u001B[0m         \u001B[0mrepl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mb2c\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb2c\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrepl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/dev/method-embedding/SourceCodeTools/nlp/string_tools.py\u001B[0m in \u001B[0;36mget_byte_to_char_map\u001B[0;34m(unicode_string)\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mbyte_offset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mchar_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcharacter\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0municode_string\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0mresponse\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbyte_offset\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mchar_offset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0;31m# print(character, byte_offset, char_offset)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SourceCodeTools]",
   "language": "python",
   "name": "conda-env-SourceCodeTools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
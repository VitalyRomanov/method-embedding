{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a variable misuse detection model one needs to implement an NLP labeling model.\n",
    "\n",
    "For example, for a funciton containing misuse\n",
    "```\n",
    "def _eq(l1, l2):\\n    return (set(l1) == set(l1))\n",
    "```\n",
    "the misuse character span is (44, 46). To do this with NLP methods, code is tokenized, and labels for tokens are generated\n",
    "```\n",
    "[def, _, eq, (, l, 1, \",\", l, 2, ):, \\n, \\t, return, (, set, (, l1, ), ==, set, (, l1, ), ), ]\n",
    "[O  , O, O , O, O, O,  O , O, O, 0 , O , O ,    O  , O, O  , O, O , O, O , O  , O, M , O, O, O\n",
    "```\n",
    "The goal is to train an NLP model that predicts those labels correctly. In this project, BILUO labeling scheme is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project\n",
    "1. Verify dataset, make sure that encoded batches are correct (misuse spans are correct). You can sample dataset and make sure that the number of errors is less than a certain threshold.\n",
    "2. Train variable misuse detection model (with finetuning and without)\n",
    "3. Verify [scoring function](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L71)\n",
    "4. Conduct a series of experiments to identify performance\n",
    "5. Analyze errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why using this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic functionality, necessary for train an NLP labeler is\n",
    "1. Loading data (implemented in this example)\n",
    "2. Tokenization, preparing labels (implemented in [`PythonBatcher.prepare_sent`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L123))\n",
    "3. Data encoding for using with ML models (implemented in [`PythonBatcher.create_batches_with_mask`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L206))\n",
    "4. Batching (implemented in [`PythonBatcher.format_batch`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L256))\n",
    "5.Â Model training (partially implemented in [`CodeBertModelTrainer2.train_model`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/codebert/codebert_train.py#L148) and extended here)\n",
    "6. Tensorboard tracking (implemented in `CodeBertModelTrainer2`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. See [installation steps](https://github.com/VitalyRomanov/method-embedding#installing-python-libraries).\n",
    "\n",
    "2. Install transformers\n",
    "```bash\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from argparse import Namespace\n",
    "from SourceCodeTools.nlp.codebert.codebert_train import CodeBertModelTrainer2, test_step, train_step_finetune, CodebertHybridModel, batch_to_torch\n",
    "from SourceCodeTools.nlp.entity.type_prediction import scorer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from time import time\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(dataset_path, partition):\n",
    "    \"\"\"\n",
    "    Read data storead as JSON records.\n",
    "    \"\"\"\n",
    "    assert partition in {\"train\", \"val\", \"test\"}\n",
    "    data_path = join(dataset_path, f\"var_misuse_seq_{partition}.json\")\n",
    "    \n",
    "    data = []\n",
    "    for line in open(data_path, \"r\"):\n",
    "        entry = json.loads(line)\n",
    "        \n",
    "        text = entry.pop(\"text\")\n",
    "        data.append((text, entry))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariableMisuseDetector(CodeBertModelTrainer2):\n",
    "    def get_trial_dir(self):\n",
    "        \"\"\"\n",
    "        Define folder name format for storing checkpoints.\n",
    "        \"\"\"\n",
    "        return os.path.join(self.output_dir, \"codebert_var_mususe\" + str(datetime.now())).replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "    \n",
    "    def train(\n",
    "            self, model, train_batches, test_batches, epochs, report_every=10, scorer=None, learning_rate=0.01,\n",
    "            learning_rate_decay=1., finetune=False, summary_writer=None, save_ckpt_fn=None, no_localization=False\n",
    "    ):\n",
    "        # all training options are specified [here](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L256)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learning_rate_decay)  # there is no learning rate decay by default\n",
    "\n",
    "        # metric history is stored here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_f1s = []\n",
    "        test_f1s = []\n",
    "\n",
    "        num_train_batches = len(train_batches)\n",
    "        num_test_batches = len(test_batches)\n",
    "\n",
    "        best_f1 = 0.\n",
    "\n",
    "        for e in range(epochs):\n",
    "            losses = []\n",
    "            ps = []\n",
    "            rs = []\n",
    "            f1s = []\n",
    "\n",
    "            start = time()\n",
    "            model.train()\n",
    "\n",
    "            for ind, batch in enumerate(tqdm(train_batches)):\n",
    "                batch_to_torch(batch, self.device)  # inspect the content of `batch`\n",
    "\n",
    "                loss, p, r, f1 = train_step_finetune(\n",
    "                    model=model, optimizer=optimizer, token_ids=batch['tok_ids'],\n",
    "                    prefix=None, suffix=None, graph_ids=None,  # keep this None\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=None,  # Keep this None\n",
    "                    scorer=scorer,\n",
    "                    finetune=finetune and e / epochs > 0.2,  # finetuning starts after 20% of training is complete\n",
    "                    vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "                losses.append(loss.cpu().item())\n",
    "                ps.append(p)\n",
    "                rs.append(r)\n",
    "                f1s.append(f1)\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Train\", loss, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Train\", p, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Train\", r, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Train\", f1, global_step=e * num_train_batches + ind)\n",
    "\n",
    "            test_alosses = []\n",
    "            test_aps = []\n",
    "            test_ars = []\n",
    "            test_af1s = []\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for ind, batch in enumerate(test_batches):\n",
    "                batch_to_torch(batch, self.device)\n",
    "                \n",
    "                test_loss, test_p, test_r, test_f1 = test_step(\n",
    "                    model=model, token_ids=batch['tok_ids'],\n",
    "                    prefix=None, suffix=None, graph_ids=None,  # keep this None\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=None,  # keep this None\n",
    "                    scorer=scorer, vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Test\", test_loss, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Test\", test_p, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Test\", test_r, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Test\", test_f1, global_step=e * num_test_batches + ind)\n",
    "                test_alosses.append(test_loss.cpu().item())\n",
    "                test_aps.append(test_p)\n",
    "                test_ars.append(test_r)\n",
    "                test_af1s.append(test_f1)\n",
    "\n",
    "            epoch_time = time() - start\n",
    "\n",
    "            train_losses.append(float(sum(losses) / len(losses)))\n",
    "            train_f1s.append(float(sum(f1s) / len(f1s)))\n",
    "            test_losses.append(float(sum(test_alosses) / len(test_alosses)))\n",
    "            test_f1s.append(float(sum(test_af1s) / len(test_af1s)))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {e}, {epoch_time: .2f} s, Train Loss: {train_losses[-1]: .4f}, Train P: {sum(ps) / len(ps): .4f}, Train R: {sum(rs) / len(rs): .4f}, Train F1: {sum(f1s) / len(f1s): .4f}, \"\n",
    "                f\"Test loss: {test_losses[-1]: .4f}, Test P: {sum(test_aps) / len(test_aps): .4f}, Test R: {sum(test_ars) / len(test_ars): .4f}, Test F1: {test_f1s[-1]: .4f}\")\n",
    "\n",
    "            if save_ckpt_fn is not None and float(test_f1s[-1]) > best_f1:\n",
    "                save_ckpt_fn()\n",
    "                best_f1 = float(test_f1s[-1])\n",
    "\n",
    "            scheduler.step(epoch=e)\n",
    "\n",
    "        return train_losses, train_f1s, test_losses, test_f1s\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        model_params = copy(self.model_params)\n",
    "\n",
    "        print(f\"\\n\\n{model_params}\")\n",
    "        lr = model_params.pop(\"learning_rate\")\n",
    "        lr_decay = model_params.pop(\"learning_rate_decay\")\n",
    "        suffix_prefix_buckets = model_params.pop(\"suffix_prefix_buckets\")  # used for another model, ignore\n",
    "\n",
    "        graph_emb = load_pkl_emb(self.graph_emb_path) if self.graph_emb_path is not None else None  # used for another model, ignore\n",
    "\n",
    "        train_batcher, test_batcher = self.get_dataloaders(word_emb=None, graph_emb=None, suffix_prefix_buckets=suffix_prefix_buckets)\n",
    "\n",
    "        codebert_model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        \n",
    "        # definition of CodebertHybridModel is at https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/codebert/codebert_train.py#L21\n",
    "        model = CodebertHybridModel(\n",
    "            codebert_model, graph_emb=None, padding_idx=0, num_classes=train_batcher.num_classes(),\n",
    "            no_graph=self.no_graph\n",
    "        )\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        trial_dir = self.get_trial_dir()  # create directory for saving checkpoints\n",
    "        os.mkdir(trial_dir)\n",
    "        self.create_summary_writer(trial_dir)\n",
    "        \n",
    "        pickle.dump(train_batcher.tagmap, open(os.path.join(trial_dir, \"tag_types.pkl\"), \"wb\"))\n",
    "\n",
    "        def save_ckpt_fn():\n",
    "            checkpoint_path = os.path.join(trial_dir, \"checkpoint\")\n",
    "            torch.save(model, open(checkpoint_path, 'wb'))\n",
    "\n",
    "        train_losses, train_f1, test_losses, test_f1 = self.train(\n",
    "            model=model, train_batches=train_batcher, test_batches=test_batcher,\n",
    "            epochs=self.epochs, learning_rate=lr,\n",
    "            scorer=lambda pred, true: scorer(pred, true, train_batcher.tagmap, no_localization=self.no_localization),  # need to verify scoring function\n",
    "            learning_rate_decay=lr_decay, finetune=self.finetune, save_ckpt_fn=save_ckpt_fn,\n",
    "            no_localization=self.no_localization\n",
    "        )\n",
    "\n",
    "        metadata = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"learning_rate_decay\": lr_decay,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"suffix_prefix_buckets\": suffix_prefix_buckets,\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"no_localization\": self.no_localization\n",
    "        }\n",
    "\n",
    "        print(\"Maximum f1:\", max(test_f1))\n",
    "\n",
    "        metadata.update(model_params)\n",
    "\n",
    "        with open(os.path.join(trial_dir, \"params.json\"), \"w\") as metadata_sink:\n",
    "            metadata_sink.write(json.dumps(metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All training options are specified [here](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L256)\n",
    "Option names are added to `args` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"variable_misuse_graph_2_percent_balanced/with_ast\"\n",
    "\n",
    "args = Namespace()\n",
    "args.__dict__.update({\n",
    "    \"learning_rate\": 1e-3,           #\n",
    "    \"max_seq_len\": 512,              # default for BERT\n",
    "    \"random_seed\": 42,               #\n",
    "    \"epochs\": 100,                   #\n",
    "    \"gpu\": -1,                       # set this to GPU id to use gpu\n",
    "    \"batch_size\": 8,                 # higher value increases memory consumption\n",
    "    \"finetune\": True,  # set this flag to enable finetuning\n",
    "    \"no_localization\": False,        # whether to solve variable misuse with, or without localization\n",
    "    \n",
    "    # do not change items below\n",
    "    \"no_graph\": True,                # used for another model\n",
    "    \"model_output\": dataset_path,    # where to store checkpoints\n",
    "    \"graph_emb_path\": None,          # used for another model\n",
    "    \"word_emb_path\": None,           # used for another model\n",
    "    \"trials\": 1,                     # setting > 1 repeats training, used to accumulate statisitcs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(dataset_path, \"train\")\n",
    "test_data = read_data(dataset_path, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"def __init__(self, pwm_pin=None, pwm_freq=50, min_ms=0.5, max_ms=2.4):\\n    assert (pwm_freq > 0), ('pwm_freq must be positive, given: %s' % str(pwm_freq))\\n    assert (min_ms > 0), ('0 min_ms must be positive, given: %s' % str(min_ms))\\n    assert (max_ms > 0), ('max_ms must be positive, given: %s' % str(max_ms))\\n    self.pwm_freq = pwm_freq\\n    self.min_ms = min_ms\\n    self.max_ms = max_ms\\n    self.pwm_pin = None\\n    if pwm_pin:\\n        self.attach(pwm_pin)\\n    self.angle = None\",\n",
       " {'replacements': [[452, 459, 870497],\n",
       "   [360, 366, 914621],\n",
       "   [226, 232, 914621],\n",
       "   [411, 415, 501975],\n",
       "   [304, 310, 904667],\n",
       "   [333, 341, 451541],\n",
       "   [94, 95, 501975],\n",
       "   [13, 17, 193236],\n",
       "   [465, 469, 557220],\n",
       "   [317, 321, 557220],\n",
       "   [385, 391, 904667],\n",
       "   [478, 482, 501975],\n",
       "   [140, 143, 65883],\n",
       "   [58, 64, 120250],\n",
       "   [144, 152, 451541],\n",
       "   [371, 375, 557220],\n",
       "   [256, 257, 501975],\n",
       "   [46, 52, 950218],\n",
       "   [19, 26, 545410],\n",
       "   [247, 253, 904667],\n",
       "   [99, 137, 501975],\n",
       "   [423, 430, 870497],\n",
       "   [33, 41, 234353],\n",
       "   [346, 350, 557220],\n",
       "   [181, 219, 501975],\n",
       "   [300, 303, 65883],\n",
       "   [167, 173, 914621],\n",
       "   [440, 444, 557220],\n",
       "   [261, 297, 501975],\n",
       "   [222, 225, 65883],\n",
       "   [83, 91, 451541],\n",
       "   [396, 400, 557220],\n",
       "   [176, 177, 501975]],\n",
       "  'entities': []})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]  # ignore `replacements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('def _prepare_verified_images(self, verify_image_url):\\n    verify_image_url._verified_images = self._verify_images(self._find_images(), verify_image_url)\\n    print(self._verified_images)',\n",
       " {'replacements': [[135, 151, 459565],\n",
       "   [163, 167, 19943],\n",
       "   [94, 98, 19943],\n",
       "   [157, 162, 940393],\n",
       "   [35, 51, 893745],\n",
       "   [58, 74, 459565],\n",
       "   [114, 118, 19943],\n",
       "   [29, 33, 187548]],\n",
       "  'entities': [[58, 74, 'misuse']]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VariableMisuseDetector(\n",
    "    train_data, test_data, params={\"learning_rate\": 1e-4, \"learning_rate_decay\": 0.99, \"suffix_prefix_buckets\": 1},\n",
    "    graph_emb_path=args.graph_emb_path, word_emb_path=args.word_emb_path,\n",
    "    output_dir=args.model_output, epochs=args.epochs, batch_size=args.batch_size, gpu_id=args.gpu,\n",
    "    finetune=args.finetune, trials=args.trials, seq_len=args.max_seq_len, no_localization=args.no_localization,\n",
    "    no_graph=args.no_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'learning_rate': 0.0001, 'learning_rate_decay': 0.99, 'suffix_prefix_buckets': 1}\n"
     ]
    }
   ],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SourceCodeTools]",
   "language": "python",
   "name": "conda-env-SourceCodeTools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

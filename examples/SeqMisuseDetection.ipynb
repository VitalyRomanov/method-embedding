{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a variable misuse detection model one needs to implement an NLP labeling model.\n",
    "\n",
    "For example, for a funciton containing misuse\n",
    "```\n",
    "def _eq(l1, l2):\\n    return (set(l1) == set(l1))\n",
    "```\n",
    "the misuse character span is (44, 46). To do this with NLP methods, code is tokenized, and labels for tokens are generated\n",
    "```\n",
    "[def, _, eq, (, l, 1, \",\", l, 2, ):, \\n, \\t, return, (, set, (, l1, ), ==, set, (, l1, ), ), ]\n",
    "[O  , O, O , O, O, O,  O , O, O, 0 , O , O ,    O  , O, O  , O, O , O, O , O  , O, M , O, O, O\n",
    "```\n",
    "The goal is to train an NLP model that predicts those labels correctly. In this project, BILUO labeling scheme is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project\n",
    "1. Verify dataset, make sure that encoded batches are correct (misuse spans are correct). You can sample dataset and make sure that the number of errors is less than a certain threshold.\n",
    "2. Train variable misuse detection model (with finetuning and without)\n",
    "3. Verify [scoring function](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L71)\n",
    "4. Conduct a series of experiments to identify performance\n",
    "5. Analyze errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why using this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic functionality, necessary for train an NLP labeler is\n",
    "1. Loading data (implemented in this example)\n",
    "2. Tokenization, preparing labels (implemented in [`PythonBatcher.prepare_sent`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L123))\n",
    "3. Data encoding for using with ML models (implemented in [`PythonBatcher.create_batches_with_mask`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L206))\n",
    "4. Batching (implemented in [`PythonBatcher.format_batch`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L256))\n",
    "5.Â Model training (partially implemented in [`CodeBertModelTrainer2.train_model`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/codebert/codebert_train.py#L148) and extended here)\n",
    "6. Tensorboard tracking (implemented in `CodeBertModelTrainer2`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. See [installation steps](https://github.com/VitalyRomanov/method-embedding#installing-python-libraries).\n",
    "\n",
    "2. Install transformers\n",
    "```bash\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from argparse import Namespace\n",
    "from SourceCodeTools.nlp.codebert.codebert_train import CodeBertModelTrainer2, test_step, train_step_finetune, CodebertHybridModel, batch_to_torch\n",
    "from SourceCodeTools.nlp.entity.type_prediction import scorer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from time import time\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(dataset_path, partition):\n",
    "    \"\"\"\n",
    "    Read data storead as JSON records.\n",
    "    \"\"\"\n",
    "    assert partition in {\"train\", \"val\", \"test\"}\n",
    "    data_path = join(dataset_path, f\"var_misuse_seq_{partition}.json\")\n",
    "    \n",
    "    data = []\n",
    "    for line in open(data_path, \"r\"):\n",
    "        entry = json.loads(line)\n",
    "        \n",
    "        text = entry.pop(\"text\")\n",
    "        data.append((text, entry))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariableMisuseDetector(CodeBertModelTrainer2):\n",
    "    def get_trial_dir(self):\n",
    "        \"\"\"\n",
    "        Define folder name format for storing checkpoints.\n",
    "        \"\"\"\n",
    "        return os.path.join(self.output_dir, \"codebert_var_mususe\" + str(datetime.now())).replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "    \n",
    "    def train(\n",
    "            self, model, train_batches, test_batches, epochs, report_every=10, scorer=None, learning_rate=0.01,\n",
    "            learning_rate_decay=1., finetune=False, summary_writer=None, save_ckpt_fn=None, no_localization=False\n",
    "    ):\n",
    "        # all training options are specified [here](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L256)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learning_rate_decay)  # there is no learning rate decay by default\n",
    "\n",
    "        # metric history is stored here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_f1s = []\n",
    "        test_f1s = []\n",
    "\n",
    "        num_train_batches = len(train_batches)\n",
    "        num_test_batches = len(test_batches)\n",
    "\n",
    "        best_f1 = 0.\n",
    "\n",
    "        for e in range(epochs):\n",
    "            losses = []\n",
    "            ps = []\n",
    "            rs = []\n",
    "            f1s = []\n",
    "\n",
    "            start = time()\n",
    "            model.train()\n",
    "\n",
    "            for ind, batch in enumerate(tqdm(train_batches)):\n",
    "                batch_to_torch(batch, self.device)  # inspect the content of `batch`\n",
    "\n",
    "                loss, p, r, f1 = train_step_finetune(\n",
    "                    model=model, optimizer=optimizer, token_ids=batch['tok_ids'],\n",
    "                    prefix=None, suffix=None, graph_ids=None,  # keep this None\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=None,  # Keep this None\n",
    "                    scorer=scorer,\n",
    "                    finetune=finetune and e / epochs > 0.2,  # finetuning starts after 20% of training is complete\n",
    "                    vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "                losses.append(loss.cpu().item())\n",
    "                ps.append(p)\n",
    "                rs.append(r)\n",
    "                f1s.append(f1)\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Train\", loss, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Train\", p, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Train\", r, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Train\", f1, global_step=e * num_train_batches + ind)\n",
    "\n",
    "            test_alosses = []\n",
    "            test_aps = []\n",
    "            test_ars = []\n",
    "            test_af1s = []\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for ind, batch in enumerate(test_batches):\n",
    "                batch_to_torch(batch, self.device)\n",
    "                \n",
    "                test_loss, test_p, test_r, test_f1 = test_step(\n",
    "                    model=model, token_ids=batch['tok_ids'],\n",
    "                    prefix=None, suffix=None, graph_ids=None,  # keep this None\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=None,  # keep this None\n",
    "                    scorer=scorer, vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Test\", test_loss, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Test\", test_p, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Test\", test_r, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Test\", test_f1, global_step=e * num_test_batches + ind)\n",
    "                test_alosses.append(test_loss.cpu().item())\n",
    "                test_aps.append(test_p)\n",
    "                test_ars.append(test_r)\n",
    "                test_af1s.append(test_f1)\n",
    "\n",
    "            epoch_time = time() - start\n",
    "\n",
    "            train_losses.append(float(sum(losses) / len(losses)))\n",
    "            train_f1s.append(float(sum(f1s) / len(f1s)))\n",
    "            test_losses.append(float(sum(test_alosses) / len(test_alosses)))\n",
    "            test_f1s.append(float(sum(test_af1s) / len(test_af1s)))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {e}, {epoch_time: .2f} s, Train Loss: {train_losses[-1]: .4f}, Train P: {sum(ps) / len(ps): .4f}, Train R: {sum(rs) / len(rs): .4f}, Train F1: {sum(f1s) / len(f1s): .4f}, \"\n",
    "                f\"Test loss: {test_losses[-1]: .4f}, Test P: {sum(test_aps) / len(test_aps): .4f}, Test R: {sum(test_ars) / len(test_ars): .4f}, Test F1: {test_f1s[-1]: .4f}\")\n",
    "\n",
    "            if save_ckpt_fn is not None and float(test_f1s[-1]) > best_f1:\n",
    "                save_ckpt_fn()\n",
    "                best_f1 = float(test_f1s[-1])\n",
    "\n",
    "            scheduler.step(epoch=e)\n",
    "\n",
    "        return train_losses, train_f1s, test_losses, test_f1s\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        model_params = copy(self.model_params)\n",
    "\n",
    "        print(f\"\\n\\n{model_params}\")\n",
    "        lr = model_params.pop(\"learning_rate\")\n",
    "        lr_decay = model_params.pop(\"learning_rate_decay\")\n",
    "        suffix_prefix_buckets = model_params.pop(\"suffix_prefix_buckets\")  # used for another model, ignore\n",
    "\n",
    "        graph_emb = load_pkl_emb(self.graph_emb_path) if self.graph_emb_path is not None else None  # used for another model, ignore\n",
    "\n",
    "        train_batcher, test_batcher = self.get_dataloaders(word_emb=None, graph_emb=None, suffix_prefix_buckets=suffix_prefix_buckets)\n",
    "\n",
    "        codebert_model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        \n",
    "        # definition of CodebertHybridModel is at https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/codebert/codebert_train.py#L21\n",
    "        model = CodebertHybridModel(\n",
    "            codebert_model, graph_emb=None, padding_idx=0, num_classes=train_batcher.num_classes(),\n",
    "            no_graph=self.no_graph\n",
    "        )\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        trial_dir = self.get_trial_dir()  # create directory for saving checkpoints\n",
    "        os.mkdir(trial_dir)\n",
    "        self.create_summary_writer(trial_dir)\n",
    "        \n",
    "        pickle.dump(train_batcher.tagmap, open(os.path.join(trial_dir, \"tag_types.pkl\"), \"wb\"))\n",
    "\n",
    "        def save_ckpt_fn():\n",
    "            checkpoint_path = os.path.join(trial_dir, \"checkpoint\")\n",
    "            torch.save(model, open(checkpoint_path, 'wb'))\n",
    "\n",
    "        train_losses, train_f1, test_losses, test_f1 = self.train(\n",
    "            model=model, train_batches=train_batcher, test_batches=test_batcher,\n",
    "            epochs=self.epochs, learning_rate=lr,\n",
    "            scorer=lambda pred, true: scorer(pred, true, train_batcher.tagmap, no_localization=self.no_localization),  # need to verify scoring function\n",
    "            learning_rate_decay=lr_decay, finetune=self.finetune, save_ckpt_fn=save_ckpt_fn,\n",
    "            no_localization=self.no_localization\n",
    "        )\n",
    "\n",
    "        metadata = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"learning_rate_decay\": lr_decay,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"suffix_prefix_buckets\": suffix_prefix_buckets,\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"no_localization\": self.no_localization\n",
    "        }\n",
    "\n",
    "        print(\"Maximum f1:\", max(test_f1))\n",
    "\n",
    "        metadata.update(model_params)\n",
    "\n",
    "        with open(os.path.join(trial_dir, \"params.json\"), \"w\") as metadata_sink:\n",
    "            metadata_sink.write(json.dumps(metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All training options are specified [here](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L256)\n",
    "Option names are added to `args` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"variable_misuse_graph_2_percent_balanced/with_ast\"\n",
    "\n",
    "args = Namespace()\n",
    "args.__dict__.update({\n",
    "    \"learning_rate\": 1e-3,           #\n",
    "    \"max_seq_len\": 512,              # default for BERT\n",
    "    \"random_seed\": 42,               #\n",
    "    \"epochs\": 100,                   #\n",
    "    \"gpu\": -1,                       # set this to GPU id to use gpu\n",
    "    \"batch_size\": 8,                 # higher value increases memory consumption\n",
    "    \"finetune\": True,  # set this flag to enable finetuning\n",
    "    \"no_localization\": False,        # whether to solve variable misuse with, or without localization\n",
    "    \n",
    "    # do not change items below\n",
    "    \"no_graph\": True,                # used for another model\n",
    "    \"model_output\": dataset_path,    # where to store checkpoints\n",
    "    \"graph_emb_path\": None,          # used for another model\n",
    "    \"word_emb_path\": None,           # used for another model\n",
    "    \"trials\": 1,                     # setting > 1 repeats training, used to accumulate statisitcs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(dataset_path, \"train\")\n",
    "test_data = read_data(dataset_path, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"def __init__(self, pwm_pin=None, pwm_freq=50, min_ms=0.5, max_ms=2.4):\\n    assert (pwm_freq > 0), ('pwm_freq must be positive, given: %s' % str(pwm_freq))\\n    assert (min_ms > 0), ('0 min_ms must be positive, given: %s' % str(min_ms))\\n    assert (max_ms > 0), ('max_ms must be positive, given: %s' % str(max_ms))\\n    self.pwm_freq = pwm_freq\\n    self.min_ms = min_ms\\n    self.max_ms = max_ms\\n    self.pwm_pin = None\\n    if pwm_pin:\\n        self.attach(pwm_pin)\\n    self.angle = None\",\n",
       " {'replacements': [[452, 459, 870497],\n",
       "   [360, 366, 914621],\n",
       "   [226, 232, 914621],\n",
       "   [411, 415, 501975],\n",
       "   [304, 310, 904667],\n",
       "   [333, 341, 451541],\n",
       "   [94, 95, 501975],\n",
       "   [13, 17, 193236],\n",
       "   [465, 469, 557220],\n",
       "   [317, 321, 557220],\n",
       "   [385, 391, 904667],\n",
       "   [478, 482, 501975],\n",
       "   [140, 143, 65883],\n",
       "   [58, 64, 120250],\n",
       "   [144, 152, 451541],\n",
       "   [371, 375, 557220],\n",
       "   [256, 257, 501975],\n",
       "   [46, 52, 950218],\n",
       "   [19, 26, 545410],\n",
       "   [247, 253, 904667],\n",
       "   [99, 137, 501975],\n",
       "   [423, 430, 870497],\n",
       "   [33, 41, 234353],\n",
       "   [346, 350, 557220],\n",
       "   [181, 219, 501975],\n",
       "   [300, 303, 65883],\n",
       "   [167, 173, 914621],\n",
       "   [440, 444, 557220],\n",
       "   [261, 297, 501975],\n",
       "   [222, 225, 65883],\n",
       "   [83, 91, 451541],\n",
       "   [396, 400, 557220],\n",
       "   [176, 177, 501975]],\n",
       "  'entities': []})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]  # ignore `replacements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('def _prepare_verified_images(self, verify_image_url):\\n    verify_image_url._verified_images = self._verify_images(self._find_images(), verify_image_url)\\n    print(self._verified_images)',\n",
       " {'replacements': [[135, 151, 459565],\n",
       "   [163, 167, 19943],\n",
       "   [94, 98, 19943],\n",
       "   [157, 162, 940393],\n",
       "   [35, 51, 893745],\n",
       "   [58, 74, 459565],\n",
       "   [114, 118, 19943],\n",
       "   [29, 33, 187548]],\n",
       "  'entities': [[58, 74, 'misuse']]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VariableMisuseDetector(\n",
    "    train_data, test_data, params={\"learning_rate\": 1e-4, \"learning_rate_decay\": 0.99, \"suffix_prefix_buckets\": 1},\n",
    "    graph_emb_path=args.graph_emb_path, word_emb_path=args.word_emb_path,\n",
    "    output_dir=args.model_output, epochs=args.epochs, batch_size=args.batch_size, gpu_id=args.gpu,\n",
    "    finetune=args.finetune, trials=args.trials, seq_len=args.max_seq_len, no_localization=args.no_localization,\n",
    "    no_graph=args.no_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'learning_rate': 0.0001, 'learning_rate_decay': 0.99, 'suffix_prefix_buckets': 1}\n"
     ]
    }
   ],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Current code works with many tokenizers. The most comparible format for storing labels is to store them as character spans. Character spans for labels are mapped to tokens with Spacy's `biluo_tags_from_offsets`. For this reason, we need to have instruments to make tokenizers compatible with Spacy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterDoc:\n",
    "    \"\"\"\n",
    "    A simple wrapper for tokens that also stores additional data such as character span adjustment and \n",
    "    tokens compatible with `biluo_tags_from_offsets`\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.adjustment_amount = 0\n",
    "        self.tokens_for_biluo_alignment = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tokens)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\".join(self.tokens)\n",
    "\n",
    "\n",
    "class CodebertAdapter:\n",
    "    \"\"\"\n",
    "    This tokenizer returns tokens in a format that can be used with `biluo_tags_from_offsets`\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        from transformers import RobertaTokenizer\n",
    "        import spacy\n",
    "\n",
    "        # create primary tokenizer\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        # create secondary tokenizer, need this to fix token alignment errors\n",
    "        self.regex_tok = create_tokenizer(\"regex\")\n",
    "        # need to have a blank spacy model for compatibility\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "\n",
    "    def primary_tokenization(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "\n",
    "    def secondary_tokenization(self, tokens):\n",
    "        # secondary tokenizer performs subtokenization \n",
    "        # example:\n",
    "        # \"(arg1\" -> \"(\", \"arg1\"\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.extend(self.regex_tok(token))\n",
    "        return new_tokens\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        Tokenization function. Example:\n",
    "            original string: 'a + b'\n",
    "            codebert tokenized: '<s>', 'a', 'Ä +', 'Ä b', '</s>'\n",
    "        \"\"\"\n",
    "        from spacy.tokens import Doc\n",
    "        tokens = self.primary_tokenization(text)\n",
    "        tokens = self.secondary_tokenization(tokens)\n",
    "        doc = Doc(self.nlp.vocab, tokens, spaces=[False] * len(tokens))\n",
    "\n",
    "        backup_tokens = doc\n",
    "        fixed_spaces = [False]\n",
    "        fixed_words = [\"<s>\"]  # add additional tokens for codebert to avoid adding them later.\n",
    "\n",
    "        for ind, t in enumerate(doc):\n",
    "            if len(t.text) > 1:\n",
    "                fixed_words.append(t.text.strip(\"Ä \"))\n",
    "            else:\n",
    "                fixed_words.append(t.text)\n",
    "            if ind != 0:\n",
    "                fixed_spaces.append(t.text.startswith(\"Ä \") and len(t.text) > 1)\n",
    "        fixed_spaces.append(False)\n",
    "        fixed_spaces.append(False)\n",
    "        fixed_words.append(\"</s>\")\n",
    "\n",
    "        assert len(fixed_spaces) == len(fixed_words)\n",
    "\n",
    "        doc = Doc(self.nlp.vocab, fixed_words, fixed_spaces)\n",
    "\n",
    "        assert len(doc) - 2 == len(backup_tokens)\n",
    "        assert len(doc.text) - 7 == len(backup_tokens.text)\n",
    "\n",
    "        final_doc = AdapterDoc([\"<s>\"] + [t.text for t in backup_tokens] + [\"</s>\"])\n",
    "        final_doc.adjustment_amount = -3\n",
    "        final_doc.tokens_for_biluo_alignment = doc\n",
    "\n",
    "        return final_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonBatcher:\n",
    "    def __init__(\n",
    "            self, data, batch_size: int, seq_len: int,\n",
    "            wordmap: Dict[str, int], *, tagmap: Optional[TagMap] = None,\n",
    "            class_weights=False, element_hash_size=1000, sort_by_length=True, tokenizer=\"spacy\", no_localization=False, **kwargs\n",
    "    ):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.class_weights = None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.no_localization = no_localization\n",
    "        self.nlp = create_tokenizer(tokenizer)\n",
    "        self.valid_sentences = 0\n",
    "        self.filtered_sentences = 0\n",
    "\n",
    "        self._create_cache()  # use cache to avoid doing the same work again\n",
    "        self._assign_ids(data)\n",
    "        self._sort_data_if_needed(sort_by_length)\n",
    "\n",
    "        self._create_tagmap_if_needed(tagmap)\n",
    "\n",
    "        self.wordpad = len(wordmap)\n",
    "\n",
    "        self.wordmap_func = lambda w: wordmap.get(w, len(wordmap))\n",
    "        self.tagmap_func = lambda t: self.tagmap.get(t, self.tagmap[\"O\"])\n",
    "\n",
    "    def __del__(self):\n",
    "#         self.sent_cache.close()\n",
    "#         self.batch_cache.close()\n",
    "\n",
    "        from shutil import rmtree\n",
    "        rmtree(self.tmp_dir, ignore_errors=True)\n",
    "\n",
    "    def _assign_ids(self, data):\n",
    "        \"\"\"\n",
    "        Assign string ids to sentences\n",
    "        \"\"\"\n",
    "        self.data = list(zip(map(str, range(len(data))), data))\n",
    "\n",
    "    def _sort_data_if_needed(self, sort_by_length):\n",
    "        \"\"\"\n",
    "        Sort by text length\n",
    "        \"\"\"\n",
    "        self.data = sorted(self.data, key=lambda x: len(x[1][0])) if sort_by_length else self.data\n",
    "\n",
    "    def _create_cache(self):\n",
    "        self.tmp_dir = get_temporary_filename()\n",
    "\n",
    "        if os.path.isdir(self.tmp_dir):\n",
    "            shutil.rmtree(self.tmp_dir)\n",
    "        os.mkdir(self.tmp_dir)\n",
    "        self.sent_cache = shelve.open(os.path.join(self.tmp_dir, \"sent_cache\"))\n",
    "        self.batch_cache = shelve.open(os.path.join(self.tmp_dir, \"batch_cache\"))\n",
    "\n",
    "    def _create_tagmap_if_needed(self, tagmap):\n",
    "        \"\"\"\n",
    "        Prepare encoder for tags\n",
    "        \"\"\"\n",
    "        if tagmap is None:\n",
    "            self.tagmap = tag_map_from_sentences(list(zip(*[self.prepare_sent(sent) for sent in self.data]))[2])\n",
    "        else:\n",
    "            self.tagmap = tagmap\n",
    "        self.tagpad = self.tagmap[\"O\"]\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.tagmap)\n",
    "\n",
    "    def prepare_sent(self, sent):\n",
    "        sent_id, sent = sent\n",
    "\n",
    "        if sent_id in self.sent_cache:\n",
    "            return self.sent_cache[sent_id]\n",
    "\n",
    "        text, annotations = sent\n",
    "\n",
    "        doc = self.nlp(text)  # perform tokenization\n",
    "        ents = annotations['entities']\n",
    "\n",
    "        tokens = doc\n",
    "        try:\n",
    "            tokens = [t.text for t in tokens]  # this is needed if spacy tokenizer is used\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if len(tokens) < self.seq_len:\n",
    "            if hasattr(doc, \"tokens_for_biluo_alignment\"):  \n",
    "                # this branch is used when codebert tokenizer is used\n",
    "                entity_adjustment_amount = doc.adjustment_amount\n",
    "                tokens_for_biluo_alignment = doc.tokens_for_biluo_alignment\n",
    "            else:\n",
    "                entity_adjustment_amount = 0\n",
    "                tokens_for_biluo_alignment = doc\n",
    "\n",
    "            # recover tag-to-token alighment\n",
    "            ents_tags = biluo_tags_from_offsets(\n",
    "                tokens_for_biluo_alignment, adjust_offsets(ents, entity_adjustment_amount),\n",
    "                self.no_localization\n",
    "            )\n",
    "            fix_incorrect_tags(ents_tags)\n",
    "\n",
    "            assert len(tokens) == len(ents_tags)\n",
    "\n",
    "            output = sent_id, tuple(tokens), tuple(ents_tags)\n",
    "            self.valid_sentences += 1\n",
    "        else:\n",
    "            output = None, None, None\n",
    "            self.filtered_sentences += 1\n",
    "\n",
    "        self.sent_cache[sent_id] = output\n",
    "        return output\n",
    "\n",
    "    # @lru_cache(maxsize=200000)\n",
    "    def create_batches_with_mask(\n",
    "            self, sent_id, sent: List[str], tags: List[str], repl: List[str] = None, unlabeled_decls: Optional[List[str]] = None\n",
    "    ):\n",
    "\n",
    "        if sent_id in self.batch_cache:\n",
    "            return self.batch_cache[sent_id]\n",
    "\n",
    "        def encode(seq, encode_func, pad):\n",
    "            blank = np.ones((self.seq_len,), dtype=np.int32) * pad\n",
    "            encoded = np.array([encode_func(w) for w in seq], dtype=np.int32)\n",
    "            blank[0:min(encoded.size, self.seq_len)] = encoded[0:min(encoded.size, self.seq_len)]\n",
    "            return blank\n",
    "\n",
    "        # input\n",
    "        s = encode(sent, self.wordmap_func, self.wordpad)\n",
    "\n",
    "        # labels\n",
    "        t = encode(tags, self.tagmap_func, self.tagpad)\n",
    "\n",
    "        assert len(s) == len(t)\n",
    "\n",
    "        no_localization_mask = np.array([tag != self.tagpad for tag in t]).astype(np.bool)\n",
    "\n",
    "        output = {\n",
    "            \"toks\": sent,\n",
    "            \"tok_ids\": s,\n",
    "            \"replacements\": repl,\n",
    "            \"tags\": t,\n",
    "            \"no_loc_mask\": no_localization_mask,\n",
    "            \"lens\": len(sent) if len(sent) < self.seq_len else self.seq_len\n",
    "        }\n",
    "\n",
    "        self.batch_cache[sent_id] = output\n",
    "        return output\n",
    "\n",
    "    def format_batch(self, batch):\n",
    "        fbatch = defaultdict(list)\n",
    "\n",
    "        for sent in batch:\n",
    "            for key, val in sent.items():\n",
    "                fbatch[key].append(val)\n",
    "\n",
    "        max_len = max(fbatch[\"lens\"])\n",
    "\n",
    "        return {\n",
    "            key: np.stack(val)[:,:max_len] if key != \"lens\" and key != \"replacements\" and key != \"toks\"\n",
    "            else (np.array(val, dtype=np.int32) if key == \"lens\" else np.array(val)) for key, val in fbatch.items()}\n",
    "\n",
    "    def generate_batches(self):\n",
    "        batch = []\n",
    "        for sent in self.data:\n",
    "            processed_sent = self.prepare_sent(sent)\n",
    "            if processed_sent[0] is None:\n",
    "                continue\n",
    "            batch.append(self.create_batches_with_mask(*processed_sent))\n",
    "            if len(batch) >= self.batch_size:\n",
    "                yield self.format_batch(batch)\n",
    "                batch = []\n",
    "        if len(batch) > 0:\n",
    "            yield self.format_batch(batch)\n",
    "        # yield self.format_batch(batch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.data) / self.batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SourceCodeTools]",
   "language": "python",
   "name": "conda-env-SourceCodeTools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

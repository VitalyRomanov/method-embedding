{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from argparse import Namespace\n",
    "from SourceCodeTools.nlp.codebert.codebert_train import CodeBertModelTrainer2, test_step, train_step_finetune, CodebertHybridModel, batch_to_torch\n",
    "from SourceCodeTools.nlp.entity.type_prediction import scorer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from time import time\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(dataset_path, partition):\n",
    "    data_path = join(dataset_path, f\"var_misuse_seq_{partition}.json\")\n",
    "    \n",
    "    data = []\n",
    "    for line in open(data_path, \"r\"):\n",
    "        entry = json.loads(line)\n",
    "        \n",
    "        text = entry.pop(\"text\")\n",
    "        data.append((text, entry))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariableMisuseDetector(CodeBertModelTrainer2):\n",
    "    def get_trial_dir(self):\n",
    "        return os.path.join(self.output_dir, \"codebert_var_mususe\" + str(datetime.now())).replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "    \n",
    "    def train(\n",
    "            self, model, train_batches, test_batches, epochs, report_every=10, scorer=None, learning_rate=0.01,\n",
    "            learning_rate_decay=1., finetune=False, summary_writer=None, save_ckpt_fn=None, no_localization=False\n",
    "    ):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learning_rate_decay)\n",
    "\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_f1s = []\n",
    "        test_f1s = []\n",
    "\n",
    "        num_train_batches = len(train_batches)\n",
    "        num_test_batches = len(test_batches)\n",
    "\n",
    "        best_f1 = 0.\n",
    "\n",
    "        for e in range(epochs):\n",
    "            losses = []\n",
    "            ps = []\n",
    "            rs = []\n",
    "            f1s = []\n",
    "\n",
    "            start = time()\n",
    "            model.train()\n",
    "\n",
    "            for ind, batch in enumerate(tqdm(train_batches)):\n",
    "                batch_to_torch(batch, self.device)\n",
    "                # token_ids, graph_ids, labels, class_weights, lengths = b\n",
    "                loss, p, r, f1 = train_step_finetune(\n",
    "                    model=model, optimizer=optimizer, token_ids=batch['tok_ids'],\n",
    "                    prefix=None, suffix=None, graph_ids=None,\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=None,\n",
    "                    # class_weights=batch['class_weights'],\n",
    "                    scorer=scorer, finetune=finetune and e / epochs > 0.6,\n",
    "                    vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "                losses.append(loss.cpu().item())\n",
    "                ps.append(p)\n",
    "                rs.append(r)\n",
    "                f1s.append(f1)\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Train\", loss, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Train\", p, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Train\", r, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Train\", f1, global_step=e * num_train_batches + ind)\n",
    "\n",
    "            test_alosses = []\n",
    "            test_aps = []\n",
    "            test_ars = []\n",
    "            test_af1s = []\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for ind, batch in enumerate(test_batches):\n",
    "                batch_to_torch(batch, self.device)\n",
    "                # token_ids, graph_ids, labels, class_weights, lengths = b\n",
    "                test_loss, test_p, test_r, test_f1 = test_step(\n",
    "                    model=model, token_ids=batch['tok_ids'],\n",
    "                    prefix=None, suffix=None, graph_ids=None,\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=None,\n",
    "                    # class_weights=batch['class_weights'],\n",
    "                    scorer=scorer, vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Test\", test_loss, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Test\", test_p, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Test\", test_r, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Test\", test_f1, global_step=e * num_test_batches + ind)\n",
    "                test_alosses.append(test_loss.cpu().item())\n",
    "                test_aps.append(test_p)\n",
    "                test_ars.append(test_r)\n",
    "                test_af1s.append(test_f1)\n",
    "\n",
    "            epoch_time = time() - start\n",
    "\n",
    "            train_losses.append(float(sum(losses) / len(losses)))\n",
    "            train_f1s.append(float(sum(f1s) / len(f1s)))\n",
    "            test_losses.append(float(sum(test_alosses) / len(test_alosses)))\n",
    "            test_f1s.append(float(sum(test_af1s) / len(test_af1s)))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {e}, {epoch_time: .2f} s, Train Loss: {train_losses[-1]: .4f}, Train P: {sum(ps) / len(ps): .4f}, Train R: {sum(rs) / len(rs): .4f}, Train F1: {sum(f1s) / len(f1s): .4f}, \"\n",
    "                f\"Test loss: {test_losses[-1]: .4f}, Test P: {sum(test_aps) / len(test_aps): .4f}, Test R: {sum(test_ars) / len(test_ars): .4f}, Test F1: {test_f1s[-1]: .4f}\")\n",
    "\n",
    "            if save_ckpt_fn is not None and float(test_f1s[-1]) > best_f1:\n",
    "                save_ckpt_fn()\n",
    "                best_f1 = float(test_f1s[-1])\n",
    "\n",
    "            scheduler.step(epoch=e)\n",
    "\n",
    "        return train_losses, train_f1s, test_losses, test_f1s\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        model_params = copy(self.model_params)\n",
    "\n",
    "        print(f\"\\n\\n{model_params}\")\n",
    "        lr = model_params.pop(\"learning_rate\")\n",
    "        lr_decay = model_params.pop(\"learning_rate_decay\")\n",
    "        suffix_prefix_buckets = model_params.pop(\"suffix_prefix_buckets\")\n",
    "\n",
    "        graph_emb = load_pkl_emb(self.graph_emb_path) if self.graph_emb_path is not None else None\n",
    "\n",
    "        train_batcher, test_batcher = self.get_dataloaders(word_emb=None, graph_emb=None, suffix_prefix_buckets=suffix_prefix_buckets)\n",
    "\n",
    "        codebert_model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        model = CodebertHybridModel(\n",
    "            codebert_model, graph_emb=None, padding_idx=0, num_classes=train_batcher.num_classes(),\n",
    "            no_graph=self.no_graph\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if self.use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        trial_dir = self.get_trial_dir()\n",
    "        os.mkdir(trial_dir)\n",
    "        self.create_summary_writer(trial_dir)\n",
    "        \n",
    "        pickle.dump(train_batcher.tagmap, open(os.path.join(trial_dir, \"tag_types.pkl\"), \"wb\"))\n",
    "\n",
    "        def save_ckpt_fn():\n",
    "            checkpoint_path = os.path.join(trial_dir, \"checkpoint\")\n",
    "            torch.save(model, open(checkpoint_path, 'wb'))\n",
    "\n",
    "        train_losses, train_f1, test_losses, test_f1 = self.train(\n",
    "            model=model, train_batches=train_batcher, test_batches=test_batcher,\n",
    "            epochs=self.epochs, learning_rate=lr,\n",
    "            scorer=lambda pred, true: scorer(pred, true, train_batcher.tagmap, no_localization=self.no_localization),\n",
    "            learning_rate_decay=lr_decay, finetune=self.finetune, save_ckpt_fn=save_ckpt_fn,\n",
    "            no_localization=self.no_localization\n",
    "        )\n",
    "\n",
    "        metadata = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"learning_rate_decay\": lr_decay,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"suffix_prefix_buckets\": suffix_prefix_buckets,\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"no_localization\": self.no_localization\n",
    "        }\n",
    "\n",
    "        print(\"Maximum f1:\", max(test_f1))\n",
    "\n",
    "        metadata.update(model_params)\n",
    "\n",
    "        with open(os.path.join(trial_dir, \"params.json\"), \"w\") as metadata_sink:\n",
    "            metadata_sink.write(json.dumps(metadata, indent=4))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/Users/LTV/Downloads/NitroShare/variable_misuse_graph_2_percent_misuse_edges\"\n",
    "\n",
    "args = Namespace()\n",
    "args.__dict__.update({\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_seq_len\": 512,\n",
    "    \"random_seed\": 42,\n",
    "    \"epochs\": 100,\n",
    "    \"gpu\": -1,\n",
    "    # do not change items below\n",
    "    \"batch_size\": 8,\n",
    "    \"no_graph\": True,\n",
    "    \"model_output\": dataset_path,\n",
    "    \"no_localization\": False,\n",
    "    \"graph_emb_path\": None,\n",
    "    \"word_emb_path\": None,\n",
    "    \"finetune\": False,\n",
    "    \"trials\": 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(dataset_path, \"train\")\n",
    "test_data = read_data(dataset_path, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VariableMisuseDetector(\n",
    "    train_data, test_data, params={\"learning_rate\": 1e-4, \"learning_rate_decay\": 0.99, \"suffix_prefix_buckets\": 1},\n",
    "    graph_emb_path=args.graph_emb_path, word_emb_path=args.word_emb_path,\n",
    "    output_dir=args.model_output, epochs=args.epochs, batch_size=args.batch_size, gpu_id=args.gpu,\n",
    "    finetune=args.finetune, trials=args.trials, seq_len=args.max_seq_len, no_localization=args.no_localization,\n",
    "    no_graph=args.no_graph\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SourceCodeTools]",
   "language": "python",
   "name": "conda-env-SourceCodeTools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

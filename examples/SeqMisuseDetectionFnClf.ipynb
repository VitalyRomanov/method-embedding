{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To train a variable misuse detection model one needs to implement an NLP labeling model.\n",
    "\n",
    "For example, for a function containing misuse\n",
    "```\n",
    "def _eq(l1, l2):\\n    return (set(l1) == set(l1))\n",
    "```\n",
    "the misuse character span is (44, 46). To do this with NLP methods, code is tokenized, and labels for tokens are generated\n",
    "```\n",
    "[def, _, eq, (, l, 1, \",\", l, 2, ):, \\n, \\t, return, (, set, (, l1, ), ==, set, (, l1, ), ), ]\n",
    "[O  , O, O , O, O, O,  O , O, O, 0 , O , O ,    O  , O, O  , O, O , O, O , O  , O, M , O, O, O\n",
    "```\n",
    "The goal is to train an NLP model that predicts those labels correctly. In this project, BILUO labeling scheme is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The goal of this project\n",
    "1. Verify dataset, make sure that encoded batches are correct (misuse spans are correct). You can sample dataset and make sure that the number of errors is less than a certain threshold.\n",
    "2. Train variable misuse detection model (with fine-tuning and without)\n",
    "3. Verify [scoring function](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L71)\n",
    "4. Conduct a series of experiments to identify performance\n",
    "5. Analyze errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Why using this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Basic functionality, necessary for train an NLP labeler is\n",
    "1. Loading data (implemented in this example)\n",
    "2. Tokenization, preparing labels (implemented in [`PythonBatcher.prepare_sent`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L123))\n",
    "3. Data encoding for using with ML models (implemented in [`PythonBatcher.create_batches_with_mask`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L206))\n",
    "4. Batching (implemented in [`PythonBatcher.format_batch`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L256))\n",
    "5.Â Model training (partially implemented in [`CodeBertModelTrainer2.train_model`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/codebert/codebert_train.py#L148) and extended here)\n",
    "6. Tensorboard tracking (implemented in `CodeBertModelTrainer2`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. See [installation steps](https://github.com/VitalyRomanov/method-embedding#installing-python-libraries).\n",
    "\n",
    "2. Install transformers\n",
    "```bash\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "from argparse import Namespace\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from transformers import RobertaModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from SourceCodeTools.nlp.codebert.codebert_train import CodeBertModelTrainer\n",
    "from SourceCodeTools.nlp.batchers.PythonBatcher import Batcher, PythonBatcher\n",
    "from SourceCodeTools.code.data.cubert_python_benchmarks.data_iterators import DataIterator\n",
    "from SourceCodeTools.nlp.entity.entity_scores import entity_scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_data_fn_clf(dataset_path, partition):\n",
    "    \"\"\"\n",
    "    Read data stored as JSON records.\n",
    "    \"\"\"\n",
    "    assert partition in {\"train\", \"val\", \"test\"}\n",
    "    data_path = join(dataset_path, f\"var_misuse_seq_{partition}.json\")\n",
    "\n",
    "    for line in open(data_path, \"r\"):\n",
    "        entry = json.loads(line)\n",
    "\n",
    "        text = entry.pop(\"text\")\n",
    "\n",
    "        entry[\"category\"] = \"misuse\" if len(entry[\"entities\"]) > 0 else \"correct\"\n",
    "\n",
    "        yield (text, entry)\n",
    "\n",
    "\n",
    "class DataIteratorFnClf(DataIterator):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return read_data_fn_clf(self._data_path, self._partition_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from SourceCodeTools.mltools.torch import to_numpy\n",
    "import torch.nn as nn\n",
    "\n",
    "class CodebertHybridModelFnClf(nn.Module):\n",
    "    def __init__(\n",
    "            self, codebert_model, graph_emb, graph_padding_idx, num_classes, dense_hidden=100, dropout=0.1,\n",
    "            bert_emb_size=768, no_graph=False\n",
    "    ):\n",
    "        super(CodebertHybridModelFnClf, self).__init__()\n",
    "\n",
    "        self.codebert_model = codebert_model\n",
    "        self.use_graph = not no_graph\n",
    "\n",
    "        if self.use_graph:\n",
    "            num_emb = graph_padding_idx + 1  # padding id is usually not a real embedding\n",
    "            graph_emb_dim = graph_emb.shape[1]\n",
    "            self.graph_emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=graph_emb_dim, padding_idx=graph_padding_idx)\n",
    "\n",
    "            pretrained_embeddings = torch.from_numpy(np.concatenate([graph_emb, np.zeros((1, graph_emb_dim))], axis=0)).float()\n",
    "            new_param = torch.nn.Parameter(pretrained_embeddings)\n",
    "            assert self.graph_emb.weight.shape == new_param.shape\n",
    "            self.graph_emb.weight = new_param\n",
    "            self.graph_emb.weight.requires_grad = False\n",
    "        else:\n",
    "            graph_emb_dim = 0\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            bert_emb_size + (graph_emb_dim if self.use_graph else 0),\n",
    "            dense_hidden\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(dense_hidden, num_classes)\n",
    "\n",
    "        self.loss_f = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, token_ids, graph_ids, mask, finetune=False):\n",
    "        if finetune:\n",
    "            x = self.codebert_model(input_ids=token_ids, attention_mask=mask).pooler_output\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                x = self.codebert_model(input_ids=token_ids, attention_mask=mask).pooler_output\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, logits, labels, mask, class_weights=None, extra_mask=None):\n",
    "        loss = self.loss_f(logits, labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def score(self, logits, labels, mask, scorer=None, extra_mask=None):\n",
    "        true_labels = labels\n",
    "        estimated_labels = logits.argmax(-1)\n",
    "        \n",
    "        acc = (estimated_labels == true_labels).sum() / len(true_labels)\n",
    "\n",
    "        return {\"Accuracy\": acc.cpu().item(), \"Prediction\": estimated_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class VariableMisuseDetector(CodeBertModelTrainer):\n",
    "\n",
    "    def set_batcher_class(self):\n",
    "        self.batcher = PythonBatcher\n",
    "\n",
    "    def set_model_class(self):\n",
    "        self.model = CodebertHybridModelFnClf\n",
    "\n",
    "    @property\n",
    "    def classes_for(self):\n",
    "        return \"labels\"\n",
    "\n",
    "    @property\n",
    "    def best_score_metric(self):\n",
    "        return \"Accuracy\"\n",
    "\n",
    "    @classmethod\n",
    "    def _format_batch(cls, batch, device):\n",
    "        key_types = {\n",
    "            'tok_ids': torch.LongTensor,\n",
    "            'tags': torch.LongTensor,\n",
    "            'hide_mask': torch.BoolTensor,\n",
    "            'no_loc_mask': torch.BoolTensor,\n",
    "            'lens': torch.LongTensor,\n",
    "            'graph_ids': torch.LongTensor,\n",
    "            'label': torch.LongTensor\n",
    "        }\n",
    "        for key, tf in key_types.items():\n",
    "            if key in batch:\n",
    "                batch[key] = tf(batch[key]).to(device)\n",
    "\n",
    "    def get_training_dir(self):\n",
    "        if not hasattr(self, \"_timestamp\"):\n",
    "            self._timestamp = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "        return Path(self.trainer_params[\"model_output\"]).joinpath(\"codebert_var_mususe_fn_clf\" + self._timestamp)\n",
    "    \n",
    "    def get_model(self, *args, **kwargs):\n",
    "        codebert_model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        model = self.model(\n",
    "            codebert_model, graph_emb=kwargs[\"graph_embedder\"],\n",
    "            graph_padding_idx=kwargs[\"graph_padding_idx\"],\n",
    "            num_classes=kwargs[\"num_classes\"],\n",
    "            no_graph=self.no_graph\n",
    "        )\n",
    "        if self.use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        if self.ckpt_path is not None:\n",
    "            ckpt_path = os.path.join(self.ckpt_path, \"checkpoint\")\n",
    "            model = self.load_checkpoint(model, ckpt_path)\n",
    "        return model\n",
    "\n",
    "    def iterate_batches(self, model, batches, epoch, num_train_batches, train_scores, scorer, train=True):\n",
    "        scores_for_averaging = defaultdict(list)\n",
    "\n",
    "        batch_count = 0\n",
    "\n",
    "        for ind, batch in enumerate(tqdm(batches, desc=f\"Epoch {epoch}\")):\n",
    "            self._format_batch(batch, self.device)\n",
    "            # Can get original tokens by calling\n",
    "            # batches.get_record_with_id(batch[\"id\"][0])\n",
    "            scores = self.make_step(\n",
    "                model=model, optimizer=self.optimizer, token_ids=batch['tok_ids'],\n",
    "                prefix=batch['prefix'], suffix=batch['suffix'],\n",
    "                graph_ids=batch['graph_ids'] if 'graph_ids' in batch else None,\n",
    "                labels=batch['label'], lengths=batch['lens'],\n",
    "                extra_mask=batch['no_loc_mask'] if self.no_localization else batch['hide_mask'],\n",
    "                # class_weights=batch['class_weights'],\n",
    "                scorer=scorer, finetune=self.finetune and epoch / self.epochs > 0.6,\n",
    "                vocab_mapping=self.vocab_mapping,\n",
    "                train=train\n",
    "            )\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "            scores[\"batch_size\"] = batch['tok_ids'].shape[0]\n",
    "            for score, value in scores.items():\n",
    "                self._write_to_summary(f\"{score}/{'Train' if train else 'Test'}\", value, epoch * num_train_batches + ind)\n",
    "                scores_for_averaging[score].append(value)\n",
    "            train_scores.append(scores_for_averaging)\n",
    "\n",
    "        return num_train_batches\n",
    "    \n",
    "    def train_model(self):\n",
    "\n",
    "        graph_emb = self._load_grap_embs()\n",
    "        word_emb = self._load_word_embs()\n",
    "\n",
    "        train_batcher, test_batcher = self.get_dataloaders(\n",
    "            word_emb, graph_emb, self.suffix_prefix_buckets, cache_dir=Path(self.data_path).joinpath(\"__cache__\")\n",
    "        )\n",
    "\n",
    "        trial_dir = self.get_training_dir()\n",
    "        trial_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logging.info(f\"Running trial: {str(trial_dir)}\")\n",
    "        self._create_summary_writer(trial_dir)\n",
    "\n",
    "        self.save_params(\n",
    "            trial_dir, {\n",
    "                \"MODEL_PARAMS\": self.model_params,\n",
    "                \"TRAINER_PARAMS\": self.trainer_params,\n",
    "                \"model_class\": self.model.__class__.__name__,\n",
    "                \"batcher_class\": self.batcher.__class__.__name__\n",
    "            }\n",
    "        )\n",
    "\n",
    "        model = self.get_model(\n",
    "            tok_embedder=word_emb, graph_embedder=graph_emb, train_embeddings=self.finetune,\n",
    "            suffix_prefix_buckets=self.suffix_prefix_buckets,\n",
    "            num_classes=train_batcher.num_classes(how=self.classes_for), seq_len=self.seq_len, no_graph=self.no_graph,\n",
    "            graph_padding_idx=None,\n",
    "            **self.model_params\n",
    "        )\n",
    "\n",
    "        def save_ckpt_fn():\n",
    "            checkpoint_path = os.path.join(trial_dir, \"checkpoint\")\n",
    "            self.save_checkpoint(model, checkpoint_path)\n",
    "\n",
    "        train_scores, test_scores, train_average_scores, test_average_scores = self.train(\n",
    "            model=model, train_batches=train_batcher, test_batches=test_batcher, epochs=self.epochs,\n",
    "            learning_rate=self.learning_rate,\n",
    "            scorer=lambda pred, true: entity_scorer(pred, true, train_batcher.tagmap,\n",
    "                                                    no_localization=self.no_localization),\n",
    "            learning_rate_decay=self.learning_rate_decay, finetune=self.finetune, save_ckpt_fn=save_ckpt_fn,\n",
    "            no_localization=self.no_localization\n",
    "        )\n",
    "\n",
    "        metadata = {\n",
    "            \"train_scores\": train_scores,\n",
    "            \"test_scores\": test_scores,\n",
    "            \"train_average_scores\": train_average_scores,\n",
    "            \"test_average_scores\": test_average_scores,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(trial_dir, \"train_data.json\"), \"w\") as metadata_sink:\n",
    "            metadata_sink.write(json.dumps(metadata, indent=4))\n",
    "\n",
    "        pickle.dump(train_batcher.tagmap, open(os.path.join(trial_dir, \"tag_types.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "All training options are specified [here](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L256)\n",
    "Option names are added to `args` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = \"variable_misuse_graph_2_percent_balanced/with_ast\"\n",
    "\n",
    "args = Namespace()\n",
    "args.__dict__.update({\n",
    "    \"learning_rate\": 1e-6,           #\n",
    "    \"learning_rate_decay\": 0.99,     #\n",
    "    \"max_seq_len\": 512,              # default for BERT\n",
    "    \"random_seed\": 42,               #\n",
    "    \"epochs\": 2,                     #\n",
    "    \"gpu\": -1,                       # set this to GPU id to use gpu\n",
    "    \"batch_size\": 8,                 # higher value increases memory consumption\n",
    "    \"finetune\": True,  # set this flag to enable finetuning\n",
    "    \"no_localization\": False,        # whether to solve variable misuse with, or without localization\n",
    "    \n",
    "    # do not change items below\n",
    "    \"data_path\": dataset_path,\n",
    "    \"no_graph\": True,                # used for another model\n",
    "    \"model_output\": dataset_path,    # where to store checkpoints\n",
    "    \"graph_emb_path\": None,          # used for another model\n",
    "    \"word_emb_path\": None,           # used for another model\n",
    "    \"trials\": 1,                     # setting > 1 repeats training, used to accumulate statisitcs\n",
    "    \"suffix_prefix_buckets\": 1,\n",
    "    \"mask_unlabeled_declarations\": False,\n",
    "    \"ckpt_path\": None\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = DataIteratorFnClf(dataset_path, \"train\")\n",
    "test_data = DataIteratorFnClf(dataset_path, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_data[0]  # ignore `replacements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = VariableMisuseDetector(\n",
    "    train_data, test_data, model_params={}, trainer_params=copy(args.__dict__)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning data: 100%|ââââââââââ| 13970/13970 [00:00<00:00, 17620.38it/s]\n",
      "Scanning data: 100%|ââââââââââ| 1462/1462 [00:00<00:00, 21613.11it/s]\n",
      "Epoch 0:  39%|ââââ      | 607/1552 [11:57<18:37,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test_data = DataIteratorFnClf(dataset_path, \"val\")\n",
    "# trainer.apply_model(test_data, \"/Users/LTV/dev/method-embeddings/examples/variable_misuse_graph_2_percent_balanced/with_ast/codebert_var_mususe_fn_clf2022-11-01_14-46-38.717526\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SourceCodeTools",
   "language": "python",
   "name": "sourcecodetools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
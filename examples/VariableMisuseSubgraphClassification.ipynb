{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ac95e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Title \n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945c3f46",
   "metadata": {
    "cellId": "0pafpqq089hs06imyofj0d8g",
    "execution_id": "9c93dc88-e371-40d6-96b9-6e397dd2d886",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9ad134",
   "metadata": {
    "cellId": "arfowdprag31hzn6iupup",
    "execution_id": "00cd8bc1-2abf-4d29-9a1a-dbd54d95af4b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "from SourceCodeTools.models.training_config import get_config, save_config, load_config\n",
    "from SourceCodeTools.code.data.dataset.Dataset import SourceGraphDataset, filter_dst_by_freq\n",
    "from SourceCodeTools.models.graph.train.sampling_multitask2 import training_procedure, SamplingMultitaskTrainer\n",
    "from SourceCodeTools.models.graph.train.objectives.NodeClassificationObjective import NodeClassifierObjective\n",
    "from SourceCodeTools.models.graph.train.objectives.SubgraphClassifierObjective import SubgraphAbstractObjective, \\\n",
    "    SubgraphClassifierObjective, SubgraphEmbeddingObjective\n",
    "from SourceCodeTools.models.graph.train.utils import get_name, get_model_base\n",
    "from SourceCodeTools.models.graph import RGGAN\n",
    "from SourceCodeTools.tabular.common import compact_property\n",
    "from SourceCodeTools.code.data.file_utils import unpersist\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "from os.path import join\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a685b9",
   "metadata": {
    "cellId": "p1vbd2almoe8o4xbjsha2i",
    "execution_id": "3d0d18ed-7a13-49ed-ad66-52e70e501e4b",
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare parameters and options\n",
    "\n",
    "Full list of options that can be added can be found in `SourceCodeTools/models/training_options.py`. They are ment to be used as arguments for cli trainer. Trainer script can be found in `SourceCodeTools/scripts/train.py`.\n",
    "\n",
    "For the task of subgraph classification the important options are:\n",
    "- `subgraph_partition` is path to subgraph-based train/val/test sets. Storead as Dataframe with subgraph id and partition mask\n",
    "- `subgraph_id_column` is a column is `common_edges` file that stores subgraph id.\n",
    "- For variable misuse task (same will apply to authorship attribution) subgraphs are created for individual functions (files for SCAA). The label is stored in `common_filecontent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee84dbf",
   "metadata": {
    "cellId": "x68rr2b8zhv1h55hg0nz",
    "execution_id": "21cdf4d1-5d0c-4d49-872a-ae3dc421d7fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_path = \"sentencepiece_bpe.model\"\n",
    "\n",
    "data_path = \"10_percent_v1\"\n",
    "subgraph_partition = join(data_path, \"partition.json.bz2\")\n",
    "filecontent_path = join(data_path, \"common_filecontent.json.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03602245",
   "metadata": {
    "cellId": "kdrwgb8pr4xkmb9fg2l7k",
    "execution_id": "606dbee0-12d0-4672-8fcb-987f08726c3c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0f6997",
   "metadata": {
    "cellId": "u8m5wmsmtsc4jlys43g8w",
    "execution_id": "321c3351-6190-4edb-bf12-3ad445249cb7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Variable misuse    35036\n",
      "Correct            35036\n",
      "Name: label, dtype: int64\n",
      "\n",
      "dev\n",
      "Correct            3754\n",
      "Variable misuse    3754\n",
      "Name: label, dtype: int64\n",
      "\n",
      "eval\n",
      "Variable misuse    18787\n",
      "Correct            18787\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = unpersist(filecontent_path)\n",
    "for part in labels['partition'].unique():\n",
    "    print(part)\n",
    "    print(labels.query('partition == @part')['label'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48e8b9",
   "metadata": {
    "cellId": "26enx0epdvd0ao7dmpz261r",
    "execution_id": "a29f9a80-f129-40c6-9be6-f0d21d0357b6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01df189",
   "metadata": {
    "cellId": "ka4x47vr7cpgilgjcoyan",
    "execution_id": "2229885e-66a6-4bed-b63d-75c212e81c55",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = get_config(\n",
    "    subgraph_id_column=\"file_id\",\n",
    "    data_path=data_path,\n",
    "    model_output_dir=data_path,\n",
    "    subgraph_partition=subgraph_partition,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    objectives=\"subgraph_clf\",\n",
    "    #use_edge_types=True,\n",
    "    gpu=-1,\n",
    "    epochs=2, \n",
    "    \n",
    "    train_frac=0.8,\n",
    "    random_seed=42, \n",
    "    \n",
    "    # model parameters\n",
    "    elem_emb_size=300,\n",
    "    node_emb_size=300,                  # *** dimensionality of node embeddings\n",
    "    h_dim=300,                           # *** should match to node dimensionality\n",
    "    n_layers=5,\n",
    "    dropout=0.1,\n",
    "    activation=\"tanh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "421b4853",
   "metadata": {
    "cellId": "f99ac2dk6fa0qpceq2iffyb",
    "execution_id": "a959159a-7369-4bb0-b829-2853f59e220a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_config(config, \"var_misuse_subgraph.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691fbf78",
   "metadata": {
    "cellId": "fn651wzuf987agk5tjmy4b",
    "execution_id": "f5806062-44f9-4b6a-bb47-dbc3d777a406",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'DATASET': {'data_path': '10_percent_v1',\n  'train_frac': 0.8,\n  'filter_edges': None,\n  'min_count_for_objectives': 5,\n  'self_loops': False,\n  'use_node_types': False,\n  'use_edge_types': False,\n  'no_global_edges': False,\n  'remove_reverse': False,\n  'custom_reverse': None,\n  'restricted_id_pool': None,\n  'random_seed': 42,\n  'subgraph_id_column': 'file_id',\n  'subgraph_partition': '10_percent_v1\\\\partition.json.bz2'},\n 'TRAINING': {'model_output_dir': '10_percent_v1',\n  'pretrained': None,\n  'pretraining_phase': 0,\n  'sampling_neighbourhood_size': 10,\n  'neg_sampling_factor': 3,\n  'use_layer_scheduling': False,\n  'schedule_layers_every': 10,\n  'elem_emb_size': 300,\n  'embedding_table_size': 200000,\n  'epochs': 2,\n  'batch_size': 128,\n  'learning_rate': 0.001,\n  'objectives': 'subgraph_clf',\n  'save_each_epoch': False,\n  'save_checkpoints': True,\n  'early_stopping': False,\n  'early_stopping_tolerance': 20,\n  'force_w2v_ns': False,\n  'use_ns_groups': False,\n  'nn_index': 'brute',\n  'metric': 'inner_prod',\n  'measure_scores': False,\n  'dilate_scores': 200,\n  'gpu': -1,\n  'external_dataset': None,\n  'restore_state': False},\n 'MODEL': {'node_emb_size': 300,\n  'h_dim': 300,\n  'n_layers': 5,\n  'use_self_loop': True,\n  'use_gcn_checkpoint': False,\n  'use_att_checkpoint': False,\n  'use_gru_checkpoint': False,\n  'num_bases': 10,\n  'dropout': 0.1,\n  'activation': 'tanh'},\n 'TOKENIZER': {'tokenizer_path': 'sentencepiece_bpe.model'}}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632d17b",
   "metadata": {
    "cellId": "zwg2nsy77lke2zhlkf3wk8",
    "execution_id": "5d3def2b-3973-476a-a885-db67b807baf0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6c34be",
   "metadata": {
    "cellId": "3842zfzh6jp2oh0p47hb3g",
    "execution_id": "4952a18f-6dca-4faf-befe-7d54f038c875",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Random state for splitting dataset is fixed\n"
     ]
    }
   ],
   "source": [
    "dataset = SourceGraphDataset(\n",
    "    **{**config[\"DATASET\"], **config[\"TOKENIZER\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691f60b",
   "metadata": {
    "cellId": "w8o2y1jvsmn3k5qlnmp8d",
    "execution_id": "459c0b0a-dac3-4c5b-8aa2-9010edf2f7da",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Declare target loading function (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ed7820",
   "metadata": {
    "cellId": "onv33y2vt60ier6qlfspo8",
    "execution_id": "024592b5-731a-4463-ac76-b5a31771b8f9",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_labels():\n",
    "    filecontent = unpersist(filecontent_path)\n",
    "    return filecontent[[\"id\", \"label\"]].rename({\"id\": \"src\", \"label\": \"dst\"}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1ef4c",
   "metadata": {
    "cellId": "3979yjouckix7kxdp4fz9",
    "execution_id": "22ff1831-62fd-4137-af18-099336d5acb7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One or several objectives could be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b773ae7",
   "metadata": {
    "cellId": "vruvsg8femgvuop8tdmqkq",
    "execution_id": "aaa60b33-deb4-49de-90ce-629f07a42ad2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(SamplingMultitaskTrainer):\n",
    "    def create_objectives(self, dataset, tokenizer_path):\n",
    "        self.objectives = nn.ModuleList()\n",
    "        \n",
    "        self.objectives.append(\n",
    "            self._create_subgraph_objective(\n",
    "                objective_name=\"VariableMisuseClf\",\n",
    "                objective_class=SubgraphClassifierObjective,\n",
    "                dataset=dataset,\n",
    "                tokenizer_path=tokenizer_path,\n",
    "                labels_fn=load_labels,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59362cf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Launch Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b303b8c4",
   "metadata": {
    "cellId": "ehoa1wxbuaq9d1dczkar",
    "execution_id": "59b5e2e8-a49d-45c8-8359-3cdd0db6c991",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir './10_percent_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8748eff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Start model training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eeccc3c",
   "metadata": {
    "cellId": "ur66zw67xepzwpxd3en2z",
    "execution_id": "39cce914-8617-4c66-aa1f-d3b744f8f0d7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtraining_procedure\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRGGAN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMODEL\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTRAINING\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_base_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mget_model_base\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTRAINING\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRGGAN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdatetime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTrainer\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\method-embedding\\SourceCodeTools\\models\\graph\\train\\sampling_multitask2.py:793\u001B[0m, in \u001B[0;36mtraining_procedure\u001B[1;34m(dataset, model_name, model_params, trainer_params, model_base_path, tokenizer_path, trainer, load_external_dataset)\u001B[0m\n\u001B[0;32m    767\u001B[0m model_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactivation\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m resolve_activation_function(model_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactivation\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    769\u001B[0m \u001B[38;5;66;03m# trainer_params = {\u001B[39;00m\n\u001B[0;32m    770\u001B[0m \u001B[38;5;66;03m#     'lr': model_params.pop('lr'),\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;66;03m#     'batch_size': args.batch_size,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m \u001B[38;5;66;03m#     \"save_each_epoch\": args.save_each_epoch\u001B[39;00m\n\u001B[0;32m    791\u001B[0m \u001B[38;5;66;03m# }\u001B[39;00m\n\u001B[1;32m--> 793\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrainer_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrestore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrainer_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrestore_state\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_embeddings_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrainer_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpretrained\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_external_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_external_dataset\u001B[49m\n\u001B[0;32m    803\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    805\u001B[0m \u001B[38;5;66;03m# try:\u001B[39;00m\n\u001B[0;32m    806\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain_all()\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\method-embedding\\SourceCodeTools\\models\\graph\\train\\sampling_multitask2.py:64\u001B[0m, in \u001B[0;36mSamplingMultitaskTrainer.__init__\u001B[1;34m(self, dataset, model_name, model_params, trainer_params, restore, device, pretrained_embeddings_path, tokenizer_path, load_external_dataset)\u001B[0m\n\u001B[0;32m     58\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m external_dataset\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_node_embedder(\n\u001B[0;32m     60\u001B[0m     dataset, tokenizer_path, n_dims\u001B[38;5;241m=\u001B[39mmodel_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh_dim\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     61\u001B[0m     pretrained_path\u001B[38;5;241m=\u001B[39mpretrained_embeddings_path, n_buckets\u001B[38;5;241m=\u001B[39mtrainer_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding_table_size\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     62\u001B[0m )\n\u001B[1;32m---> 64\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_objectives\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m restore:\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrestore_from_checkpoint(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_base_path)\n",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36mTrainer.create_objectives\u001B[1;34m(self, dataset, tokenizer_path)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_objectives\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset, tokenizer_path):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjectives \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList()\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjectives\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m----> 6\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_subgraph_objective\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m            \u001B[49m\u001B[43mobjective_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mVariableMisuseClf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m            \u001B[49m\u001B[43mobjective_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSubgraphClassifierObjective\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtokenizer_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlabels_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\method-embedding\\SourceCodeTools\\models\\graph\\train\\sampling_multitask2.py:146\u001B[0m, in \u001B[0;36mSamplingMultitaskTrainer._create_subgraph_objective\u001B[1;34m(self, objective_name, objective_class, dataset, labels_fn, tokenizer_path, subgraph_mapping, subgraph_partition, masker)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_subgraph_objective\u001B[39m(\n\u001B[0;32m    141\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m, objective_name, objective_class, dataset, labels_fn, tokenizer_path, subgraph_mapping\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    142\u001B[0m         subgraph_partition\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    143\u001B[0m         masker\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    144\u001B[0m ):\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m subgraph_mapping \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 146\u001B[0m         subgraph_mapping \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubgraph_mapping\u001B[49m\n\u001B[0;32m    148\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m subgraph_partition \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    149\u001B[0m         subgraph_partition \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39msubgraph_partition\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\method-embedding\\SourceCodeTools\\code\\data\\dataset\\Dataset.py:1022\u001B[0m, in \u001B[0;36mSourceGraphDataset.subgraph_mapping\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1019\u001B[0m         subgraph_mapping[subgraph_id] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[0;32m   1021\u001B[0m     subgraph_dict \u001B[38;5;241m=\u001B[39m subgraph_mapping[subgraph_id]\n\u001B[1;32m-> 1022\u001B[0m     \u001B[43madd_item\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubgraph_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1023\u001B[0m     add_item(subgraph_dict, dst)\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subgraph_id, subgraph_dict \u001B[38;5;129;01min\u001B[39;00m subgraph_mapping\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\method-embedding\\SourceCodeTools\\code\\data\\dataset\\Dataset.py:1010\u001B[0m, in \u001B[0;36mSourceGraphDataset.subgraph_mapping.<locals>.add_item\u001B[1;34m(subgraph_dict, node_id)\u001B[0m\n\u001B[0;32m   1009\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_item\u001B[39m(subgraph_dict, node_id):\n\u001B[1;32m-> 1010\u001B[0m     type_ \u001B[38;5;241m=\u001B[39m \u001B[43mid2type\u001B[49m[node_id]\n\u001B[0;32m   1012\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m type_ \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m subgraph_dict:\n\u001B[0;32m   1013\u001B[0m         subgraph_dict[type_] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "training_procedure(\n",
    "    dataset, \n",
    "    model_name=RGGAN, \n",
    "    model_params=config[\"MODEL\"],\n",
    "    trainer_params=config[\"TRAINING\"],\n",
    "    model_base_path=get_model_base(config[\"TRAINING\"], get_name(RGGAN, str(datetime.now()))),\n",
    "    trainer=Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9c80e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SourceCodeTools",
   "language": "python",
   "name": "sourcecodetools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notebookId": "0b349f50-6796-4a42-8a61-23bcf9f52520",
  "notebookPath": "method-embedding/examples/Subgraph Classification.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
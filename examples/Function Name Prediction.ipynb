{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "from SourceCodeTools.models.training_config import get_config, save_config, load_config\n",
    "from SourceCodeTools.code.data.dataset.Dataset import SourceGraphDataset, filter_dst_by_freq\n",
    "from SourceCodeTools.models.graph.train.sampling_multitask2 import training_procedure, SamplingMultitaskTrainer\n",
    "from SourceCodeTools.models.graph.train.objectives.NodeClassificationObjective import NodeClassifierObjective\n",
    "from SourceCodeTools.models.graph.train.utils import get_name, get_model_base\n",
    "from SourceCodeTools.models.graph import RGGAN\n",
    "from SourceCodeTools.tabular.common import compact_property\n",
    "from SourceCodeTools.code.data.file_utils import unpersist\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from torch import nn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare parameters and options\n",
    "\n",
    "Full list of options that can be added can be found in `SourceCodeTools/models/training_options.py`. They are ment to be used as arguments for cli trainer. Trainer script can be found in `SourceCodeTools/scripts/train.py`.\n",
    "\n",
    "There are a lot of parameters. Ones that might be of interest are marked with `***`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DATASET': {'custom_reverse': None,\n",
       "  'data_path': 'large_graph',\n",
       "  'filter_edges': None,\n",
       "  'min_count_for_objectives': 5,\n",
       "  'no_global_edges': False,\n",
       "  'random_seed': 42,\n",
       "  'remove_reverse': False,\n",
       "  'restricted_id_pool': None,\n",
       "  'self_loops': False,\n",
       "  'train_frac': 0.8,\n",
       "  'use_edge_types': True,\n",
       "  'use_node_types': False},\n",
       " 'MODEL': {'activation': 'tanh',\n",
       "  'dropout': 0.2,\n",
       "  'h_dim': 100,\n",
       "  'n_layers': 5,\n",
       "  'node_emb_size': 100,\n",
       "  'num_bases': 10,\n",
       "  'use_att_checkpoint': True,\n",
       "  'use_gcn_checkpoint': True,\n",
       "  'use_gru_checkpoint': True,\n",
       "  'use_self_loop': True},\n",
       " 'TRAINING': {'batch_size': 1024,\n",
       "  'dilate_scores': 200,\n",
       "  'early_stopping': False,\n",
       "  'early_stopping_tolerance': 20,\n",
       "  'elem_emb_size': 100,\n",
       "  'embedding_table_size': 200000,\n",
       "  'epochs': 10,\n",
       "  'external_dataset': None,\n",
       "  'force_w2v_ns': True,\n",
       "  'gpu': -1,\n",
       "  'learning_rate': 0.001,\n",
       "  'measure_scores': True,\n",
       "  'metric': 'inner_prod',\n",
       "  'model_output_dir': 'large_graph',\n",
       "  'neg_sampling_factor': 1,\n",
       "  'nn_index': 'brute',\n",
       "  'objectives': 'node_clf',\n",
       "  'pretrained': None,\n",
       "  'pretraining_phase': 0,\n",
       "  'restore_state': False,\n",
       "  'sampling_neighbourhood_size': 10,\n",
       "  'save_checkpoints': False,\n",
       "  'save_each_epoch': False,\n",
       "  'schedule_layers_every': 10,\n",
       "  'tokenizer_path': 'sentencepiece_bpe.model',\n",
       "  'use_layer_scheduling': False,\n",
       "  'use_ns_groups': False}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Random state for splitting dataset is fixed\n"
     ]
    }
   ],
   "source": [
    "dataset = SourceGraphDataset(\n",
    "    **config[\"DATASET\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare target loading function (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_type_prediction():\n",
    "    from SourceCodeTools.code.data.dataset.reader import load_data\n",
    "    \n",
    "    nodes, edges = dataset.nodes, dataset.edges\n",
    "    \n",
    "    type_ann = unpersist(\"large_graph/type_annotations.json.bz2\").query(\"src in @node_ids\", local_dict={\"node_ids\": nodes[\"id\"]})\n",
    "    \n",
    "    norm = lambda x: x.strip(\"\\\"\").strip(\"'\").split(\"[\")[0].split(\".\")[-1]\n",
    "\n",
    "    type_ann[\"dst\"] = type_ann[\"dst\"].apply(norm)\n",
    "    type_ann = filter_dst_by_freq(type_ann, config[\"DATASET\"][\"min_count_for_objectives\"])\n",
    "    type_ann = type_ann[[\"src\", \"dst\"]]\n",
    "\n",
    "    return type_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define objectives\n",
    "\n",
    "Currenlty objectives for node classification (`NodeClassifierObjective`), and name-based node embedding training `SubwordEmbedderObjective`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"examples/figures/img1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One or several objectives could be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(SamplingMultitaskTrainer):\n",
    "    def create_objectives(self, dataset, tokenizer_path):\n",
    "        self.objectives = nn.ModuleList()\n",
    "        \n",
    "#         self.objectives.append(\n",
    "#             NodeClassifierObjective(\n",
    "#                 \"NodeTypeClassifier\",\n",
    "#                 self.graph_model, self.node_embedder, dataset.nodes,\n",
    "#                 dataset.load_node_classes,                                              # need to define this function\n",
    "#                 self.device, self.sampling_neighbourhood_size, self.batch_size,\n",
    "#                 tokenizer_path=tokenizer_path, target_emb_size=self.elem_emb_size,\n",
    "#                 masker=dataset.create_node_clf_masker(),                                # this is needed only for node type classification\n",
    "#                 measure_scores=self.trainer_params[\"measure_scores\"],\n",
    "#                 dilate_scores=self.trainer_params[\"dilate_scores\"]\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "        self.objectives.append(\n",
    "            NodeClassifierObjective(\n",
    "                \"TypeAnnPrediction\",\n",
    "                self.graph_model, self.node_embedder, dataset.nodes,\n",
    "                load_type_prediction,                                                   # need to define this function\n",
    "                self.device, self.sampling_neighbourhood_size, self.batch_size,\n",
    "                tokenizer_path=tokenizer_path, target_emb_size=self.elem_emb_size, \n",
    "                masker=None,                                                            # masker is not needed here\n",
    "                measure_scores=self.trainer_params[\"measure_scores\"],\n",
    "                dilate_scores=self.trainer_params[\"dilate_scores\"]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with -9)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"large_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes 324218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s]/Users/LTV/Soft/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "training_procedure(\n",
    "    dataset, \n",
    "    model_name=RGGAN, \n",
    "    model_params=config[\"MODEL\"],\n",
    "    trainer_params=config[\"TRAINING\"],\n",
    "    model_base_path=get_model_base(config[\"TRAINING\"], get_name(RGGAN, str(datetime.now()))),\n",
    "    trainer=Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SourceCodeTools",
   "language": "python",
   "name": "sourcecodetools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
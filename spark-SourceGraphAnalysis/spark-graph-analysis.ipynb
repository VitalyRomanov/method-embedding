{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages = [\"graphframes:graphframes:0.8.1-spark3.0-s_2.12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.91.67.150:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1610433271365)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.graphframes._\n",
       "import org.apache.log4j.Logger\n",
       "import org.apache.log4j.Level\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.graphframes._\n",
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3e05b954\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    "      .appName(\"GraphAnalysis\").master(\"local[4]\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nodes_path: String = /Users/LTV/Downloads/NitroShare/with_ast/common_nodes.parquet\n",
       "edges_path: String = /Users/LTV/Downloads/NitroShare/with_ast/common_edges.parquet\n",
       "output_path: String = /Users/LTV/Downloads/NitroShare/with_ast/largest_component\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nodes_path = \"/Users/LTV/Downloads/NitroShare/with_ast/common_nodes.parquet\"\n",
    "val edges_path = \"/Users/LTV/Downloads/NitroShare/with_ast/common_edges.parquet\"\n",
    "val output_path = \"/Users/LTV/Downloads/NitroShare/with_ast/largest_component\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original graph\n",
      "Nodes: 1637944, Edges: 8687701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nodes: org.apache.spark.sql.DataFrame = [id: bigint, type: string ... 2 more fields]\n",
       "edges: org.apache.spark.sql.DataFrame = [rel_id: bigint, rel_type: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert(nodes_path.endsWith(\".parquet\"), \"Nodes should be stored in parquet format\")\n",
    "assert(edges_path.endsWith(\".parquet\"), \"Edges should be stored in parquet format\")\n",
    "\n",
    "val nodes = spark.read.load(nodes_path)\n",
    "val edges = spark.read.load(edges_path)\n",
    "  .withColumnRenamed(\"source_node_id\", \"src\")\n",
    "  .withColumnRenamed(\"target_node_id\", \"dst\")\n",
    "  .withColumnRenamed(\"id\", \"rel_id\")\n",
    "  .withColumnRenamed(\"type\", \"rel_type\")\n",
    "\n",
    "println(s\"Original graph\")\n",
    "println(s\"Nodes: ${nodes.count()}, Edges: ${edges.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original graph\n",
      "Nodes: 1637944, Edges: 8687701\n",
      "[3,module,__future__,null]\n",
      "[4,global_variable,__future__.absolute_import,null]\n",
      "[5,global_variable,__future__.division,null]\n",
      "[6,global_variable,__future__.print_function,null]\n",
      "[7,module,sys,null]\n",
      "[8,module,types,null]\n",
      "[16,non_indexed_symbol,builtins,null]\n",
      "[17,class,builtins.set,null]\n",
      "[18,function,builtins.set.add,null]\n",
      "[19,function,builtins.id,null]\n",
      "[4,defines,3045236,3045237]\n",
      "[6,defines,3045237,3045238]\n",
      "[8,defines,3045238,3045239]\n",
      "[10,imports,3045239,207]\n",
      "[12,defines,3045239,3045240]\n",
      "[14,defines,207,447]\n",
      "[15,calls,3045239,447]\n",
      "[17,uses,3045239,21]\n",
      "[19,imports,3045239,208]\n",
      "[21,imports,3045239,209]\n"
     ]
    }
   ],
   "source": [
    "println(s\"Original graph\")\n",
    "println(s\"Nodes: ${nodes.count()}, Edges: ${edges.count()}\")\n",
    "nodes.take(10).foreach(println)\n",
    "edges.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping isolated\n",
      "Nodes: 1637944, Edges: 8687701\n",
      "[474,non_indexed_symbol,builtins.BaseException,null]\n",
      "[964,global_variable,typing.Union,null]\n",
      "[1950,Op,Eq,188]\n",
      "[5409,subword,PREFIX,1432]\n",
      "[7747,subword,▁arg,2315]\n",
      "[15057,subword,sep,361]\n",
      "[15663,subword,083,7384]\n",
      "[17499,subword,▁tf,7536]\n",
      "[35484,non_indexed_symbol,pwd.struct_passwd.pw_name,null]\n",
      "[51428,module,logging.handlers,null]\n",
      "[4,defines,3045236,3045237]\n",
      "[6,defines,3045237,3045238]\n",
      "[8,defines,3045238,3045239]\n",
      "[10,imports,3045239,207]\n",
      "[12,defines,3045239,3045240]\n",
      "[14,defines,207,447]\n",
      "[15,calls,3045239,447]\n",
      "[17,uses,3045239,21]\n",
      "[19,imports,3045239,208]\n",
      "[21,imports,3045239,209]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "g: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, type: string ... 2 more fields], e:[src: bigint, dst: bigint ... 2 more fields])\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val g = GraphFrame(nodes, edges).dropIsolatedVertices()\n",
    "\n",
    "println(s\"After dropping isolated\")\n",
    "println(s\"Nodes: ${g.vertices.count()}, Edges: ${g.edges.count()}\")\n",
    "\n",
    "g.vertices.take(10).foreach(println)\n",
    "g.edges.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cc_result: org.apache.spark.sql.DataFrame = [id: bigint, type: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.setCheckpointDir(\"temp\")\n",
    "val cc_result = g.connectedComponents.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g_cc: org.graphframes.GraphFrame = GraphFrame(v:[id: bigint, type: string ... 3 more fields], e:[src: bigint, dst: bigint ... 2 more fields])\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val g_cc = GraphFrame(cc_result, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the largest component\n",
      "Nodes: 1637944, Edges: 8687701\n"
     ]
    }
   ],
   "source": [
    "println(s\"In the largest component\")\n",
    "println(s\"Nodes: ${g_cc.vertices.count()}, Edges: ${g_cc.edges.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[474,non_indexed_symbol,builtins.BaseException,null,3]\n",
      "[964,global_variable,typing.Union,null,3]\n",
      "[1950,Op,Eq,188,3]\n",
      "[5409,subword,PREFIX,1432,3]\n",
      "[7747,subword,▁arg,2315,3]\n",
      "[15057,subword,sep,361,3]\n",
      "[15663,subword,083,7384,3]\n",
      "[17499,subword,▁tf,7536,3]\n",
      "[35484,non_indexed_symbol,pwd.struct_passwd.pw_name,null,3]\n",
      "[51428,module,logging.handlers,null,3]\n",
      "[4,defines,3045236,3045237]\n",
      "[6,defines,3045237,3045238]\n",
      "[8,defines,3045238,3045239]\n",
      "[10,imports,3045239,207]\n",
      "[12,defines,3045239,3045240]\n",
      "[14,defines,207,447]\n",
      "[15,calls,3045239,447]\n",
      "[17,uses,3045239,21]\n",
      "[19,imports,3045239,208]\n",
      "[21,imports,3045239,209]\n"
     ]
    }
   ],
   "source": [
    "g_cc.vertices.take(10).foreach(println)\n",
    "g_cc.edges.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "component_count: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [component: bigint, count: bigint]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val component_count = g_cc.vertices.groupBy(\"component\").count().sort(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row: Array[org.apache.spark.sql.Row] = Array([3,1637944])\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val row = component_count.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Long = 3\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row(0).getAs[Long](\"component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Long = 1637944\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row(0).getAs[Long](\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com_local: Array[org.apache.spark.sql.Row] = Array([3,1637944])\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val com_local = component_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
